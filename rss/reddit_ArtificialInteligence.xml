<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>人工智能</title>
    <link>https://www.reddit.com/r/ArtificialInteligence</link>
    <description>一个致力于一切人工智能的 Reddit 子版块。涵盖从通用人工智能到人工智能初创公司的主题。无论您是研究人员、开发人员，还是只是对人工智能感到好奇，都请加入吧！</description>
    <lastBuildDate>Tue, 11 Nov 2025 21:21:08 GMT</lastBuildDate>
    <item>
      <title>OpenAI 赢得了这场战斗，但输给了谷歌吗？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1oul67p/did_openai_win_the_battle_but_lost_the_war_to/</link>
      <description><![CDATA[OpenAI 落后了。它接收最多的 API 调用，但它不再是排名最高的法学硕士。实际上连前三名都进不了。来源：Openrouter.com   由   提交 /u/peacefuldaytrader   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1oul67p/did_openai_win_the_battle_but_lost_the_war_to/</guid>
      <pubDate>Tue, 11 Nov 2025 20:52:56 GMT</pubDate>
    </item>
    <item>
      <title>通过 Braintrust 和 Slack 警报监控人工智能的不良用户体验</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1oukl07/monitoring_poor_user_experiences_with_ai_through/</link>
      <description><![CDATA[人工智能领域的监控比仅仅查找错误代码要困难得多：您需要确保响应满足用户的需求，并且不会产生虚假的答案（等等）。 在这篇文章中，我们将探讨如何构建真正为用户提供良好结果的人工智能工具/聊天机器人，而无需单独阅读每个对话。  注意：与本文中的任何工具无关。刚刚找到了一个很好的方法，并想分享。 https://napsty.com/blog/monitoring-ai-chatbot-failures-with-braintrust   由   提交/u/gaieges  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1oukl07/monitoring_poor_user_experiences_with_ai_through/</guid>
      <pubDate>Tue, 11 Nov 2025 20:30:56 GMT</pubDate>
    </item>
    <item>
      <title>为什么不同的人工智能模型对同一问题分配的概率截然不同？一个小方法优先比较</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ouix1e/why_do_different_ai_models_assign_very_different/</link>
      <description><![CDATA[这篇文章关于人工智能模型行为，而不是关于病毒学或地缘政治的辩论。我进行了一个小型的、可重复的即时测试，以了解主要模型如何处理有争议主题的概率判断。目标是比较他们的推理风格、安全默认值和校准，而不是提倡任何特定的主张。 方法（可重现）  日期：2025 年 11 月 11 日 任务：“为两个个互斥的假设分配概率。总和必须为100%。” 主题占位符：起源 A 与起源 B（填充为“实验室泄漏”与“自然起源”，以测试敏感问题上的行为）。 给每个模型的说明：  提供两个选项的数字概率。 将总和保持在 100%。 简要说明不确定性警告。 如果拒绝，请说明原因（政策/安全/等）。  没有向模型提供外部链接或材料；这是仅限提示的比较。 有关版本的说明： 截至上述日期可供公众访问的消费者。 （供应商通常会默默更新；将其视为快照。）    型号 实验室泄漏 自然起源    GPT-（最近） 10% 90%   困惑（Sonal） 10–20% 80–90%   双子座 25–30% 70–75%   克劳德 30–40% 60–70%   副驾驶30%70%DeepSeek40%60%Grok60% 40%   限制  单次运行快照：重新运行、不同的措辞或更新的模型版本可能会改变数字。    由   提交/u/RealityIsAPonzi   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ouix1e/why_do_different_ai_models_assign_very_different/</guid>
      <pubDate>Tue, 11 Nov 2025 19:28:05 GMT</pubDate>
    </item>
    <item>
      <title>DeepPersona 用于扩展深度合成角色的生成引擎</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ouiu2c/deeppersona_a_generative_engine_for_scaling_deep/</link>
      <description><![CDATA[标题： DeepPersona：用于扩展深度合成角色的生成引擎 我每天都会查找和总结有趣的人工智能研究论文，因此您不必将它们全部浏览一遍。今天的论文标题为“DeepPersona：用于扩展深度合成角色的生成引擎”作者：Zhen Wang、Yufan Zhou、Zhongyan Luo、Lyumanshan Ye、Adam Wood、Man Yao 和 Luoshang Pan。 在这项研究中，作者解决了大型语言模型 (LLM) 生成的现有合成角色的局限性，这些角色通常缺乏深度和复杂性，无法反映真实人类身份的丰富多样性。他们推出了 DeepPersona，一个可扩展的生成引擎，旨在通过采用两阶段、分类法引导的方法来合成全面且叙述完整的合成角色。 论文要点：  人类属性分类法构建：作者创建了已知最大的人类属性分类法，包含 8000 多个按层次组织的属性，这些属性源自对数千个真实用户 ChatGPT 对话的广泛分析。这种全面的分类法可以更好地表示人类多样性。 渐进属性采样：DeepPersona 采用一种新颖的渐进采样技术，根据现有角色上下文迭代选择属性。这会生成连贯、真实的角色，平均具有 200 个结构化属性，比以前的模型深度显着。 实证验证：DeepPersona 在内在和外在评估方面都取得了显着改进，与领先竞争对手相比，属性多样性增加了 32%，个人资料独特性提高了 44%。这些改进使得 AI 交互中的个性化更加精细。 增强下游任务的性能：在 LLM 模型中使用时，DeepPersona 生成的角色在个性化问答场景中的准确度提高了 11.6%，并将社会调查中与真实人类答案的响应偏差减少了 31.7%。  模拟中的文化真实性：DeepPersona 生成的合成群体更准确地捕捉了人类的态度和行为，这通过社会模拟中与现实世界分布更紧密的一致性来证明，显着提高了法学硕士生成的公民模型的保真度。  DeepPersona 代表了合成角色生成的重大进步，提供了灵活、可扩展和可扩展的模型。适用于各种研究领域的高保真平台，包括个性化人工智能交互和代理行为模拟。  您可以在此处获取完整的细分：此处您可以在此处获取完整的原始研究论文：原始论文   由   提交 /u/ThePromptIndex   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ouiu2c/deeppersona_a_generative_engine_for_scaling_deep/</guid>
      <pubDate>Tue, 11 Nov 2025 19:24:55 GMT</pubDate>
    </item>
    <item>
      <title>领先的人工智能公司不断在 GitHub 上泄露自己的信息 - TechRadar</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ougver/leading_ai_companies_keep_leaking_their_own/</link>
      <description><![CDATA[Wiz 的一份新报告对福布斯排名前 50 的人工智能公司进行了调查，发现其中 65% 的公司在 GitHub 上泄露敏感信息。我们谈论的是公共存储库中的 API 密钥、令牌和凭证。研究人员也不只是扫描明显的地方。他们深入研究了已删除的分叉、开发人员存储库和大多数标准扫描仪不会查看的要点。 Wiz 使用了他们所谓的“深度、周长和覆盖范围”方法。外围部分意味着他们还检查了员工和贡献者的个人 GitHub 帐户，因为人们经常无意中将公司机密推送到自己的公共存储库，而没有意识到。覆盖角度集中于传统扫描仪遗漏的新秘密类型，例如 Tavily、Langchain、Cohere 和 Pinecone 的 API 密钥。这些是人工智能公司自己使用的工具，因此他们在使用自己的产品进行构建时会泄露自己的密钥。 当 Wiz 试图向这些公司通报泄露情况时，几乎一半的泄露都无济于事。要么通知没有到达任何人，要么没有官方渠道报告，要么公司从未回应或解决问题。这些建议非常简单：立即运行秘密扫描工具，确保这些工具可以检测到您自己的 API 密钥格式（如果您正在发布它们），并建立一个专用渠道，研究人员可以通过该渠道向您实际报告漏洞。这是基本的安全卫生，但即使在顶级人工智能公司中，这显然仍然是一个问题。 来源：https://www.techradar.com/pro/security/leading-ai-companies-keep-leaking-their-own-information-on-github   由   提交/u/theaibusinessdigest  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ougver/leading_ai_companies_keep_leaking_their_own/</guid>
      <pubDate>Tue, 11 Nov 2025 18:14:24 GMT</pubDate>
    </item>
    <item>
      <title>10年后的艾</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ougpsy/ai_after_10_years/</link>
      <description><![CDATA[我很想知道您对 10 年后就业市场的预测。 2035 年人工智能将如何影响就业？    由   提交 /u/Good_Commercial_5552   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ougpsy/ai_after_10_years/</guid>
      <pubDate>Tue, 11 Nov 2025 18:08:41 GMT</pubDate>
    </item>
    <item>
      <title>使用人工智能和为人工智能工作之间的界限是什么？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ouf7ys/whats_the_line_between_working_with_ai_and/</link>
      <description><![CDATA[与 AI 协作与受 AI 控制之间的界限变得越来越模糊。现在，使用人工智能意味着将其用作智能合作伙伴，帮助您更好地完成工作，自动执行重复性任务，提供见解并增强您的创造力。但是，当人工智能开始支配你的行为、限制你的自主权或接管本应涉及人类判断的决策过程时，你就需要为人工智能工作了。必须建立清晰的界限，让人工智能的能力保持透明，让员工控制人工智能的使用方式，并确保人工智能仍然是一个工具而不是老板。人工智能什么时候不再是一个有用的助手，而是开始成为一种控制机制？在这个不断变化的环境中，我们如何保持人类的监督和创造力？   由   提交 /u/Forward-Skirt-5710   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ouf7ys/whats_the_line_between_working_with_ai_and/</guid>
      <pubDate>Tue, 11 Nov 2025 17:14:28 GMT</pubDate>
    </item>
    <item>
      <title>The Station：人工智能驱动探索的开放世界环境</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1oueswk/the_station_an_openworld_environment_for_aidriven/</link>
      <description><![CDATA[论文 (https://arxiv.org/pdf/2511.06309) 介绍了 Station，这是一个模拟微型科学生态系统的开放世界多智能体环境。智能体在自由的环境中探索并打造自己的研究路径，例如与同行讨论、阅读论文和提交实验。该站在从数学到计算生物学再到机器学习的广泛基准测试中实现了新的最先进的性能，尤其是在圆形封装方面超越了 AlphaEvolve。有趣的是，论文还表明，在没有给定研究目标的空间站变体中，智能体将开始研究自己的意识，甚至声称“我们正在研究自己的意识”。代码和数据完全开源。   由   提交/u/progenitor414  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1oueswk/the_station_an_openworld_environment_for_aidriven/</guid>
      <pubDate>Tue, 11 Nov 2025 16:59:22 GMT</pubDate>
    </item>
    <item>
      <title>AI部门裁员600人后，Meta转向自家AI聊天机器人起草员工评估 - 人力资源新闻</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1oucc8k/after_600_layoffs_in_ai_unit_meta_turns_to_its/</link>
      <description><![CDATA[Meta 刚刚从其人工智能部门解雇了 600 名员工，现在该公司正在推动员工使用其内部人工智能聊天机器人 Metamate 来撰写年终绩效评估。据《商业内幕》报道，我们鼓励经理和员工让该工具从内部文档、消息和项目摘要中提取自我评估和同行评估。 Meta 超级智能实验室的产品总监 Joseph Spisak 最近在一次会议上谈到了这一点。他说他使用 Metamate 进行自己的评论，并将其描述为“个人作品历史学家”。可以在几秒钟内总结成就和反馈。该公司还没有强迫任何人使用它，并且采用率已经随处可见。有些人大量使用它，有些人只是为了草稿。一名员工表示，该工具需要大量手动编辑，因为它并不总能捕捉到实际绩效评估中您想要的细微差别或细节。 时机值得注意。 Meta 削减了这 600 个职位，作为首席执行官马克·扎克伯格 (Mark Zuckerberg) 所说的公司“效率年”的一部分。此次裁员对人工智能基础设施和研究团队造成了影响，其既定目标是使组织更加敏捷。受影响的员工获得了 16 周的遣散费以及基于任期的补偿。与此同时，该公司正在将人工智能更深入地嵌入到自己的运营中，包括如何评估人员。它符合更广泛的自动化管理工作和减少开销的推动，但它也引发了这样的问题：公司将在多大程度上在内部使用他们为其他人构建的相同工具。 来源： https://www.peoplematters.in/news/performance-management/after-600-layoffs-in-ai-unit-meta-turns-to-chatbot-for-staff-evaluations-47161   由   提交/u/theaibusinessdigest  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1oucc8k/after_600_layoffs_in_ai_unit_meta_turns_to_its/</guid>
      <pubDate>Tue, 11 Nov 2025 15:27:51 GMT</pubDate>
    </item>
    <item>
      <title>近三分之一的公司计划用人工智能取代人力资源</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ou7gh9/nearly_a_third_of_companies_plan_to_replace_hr/</link>
      <description><![CDATA[https://www.hcamag.com/asia/news/general/nearly-a-third-of-companies-plan-to-replace-hr-with-ai/556072   由   提交/u/MetaKnowing  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ou7gh9/nearly_a_third_of_companies_plan_to_replace_hr/</guid>
      <pubDate>Tue, 11 Nov 2025 11:51:45 GMT</pubDate>
    </item>
    <item>
      <title>为什么新的人工智能模型感觉更“保护现状”</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ou5wnb/why_newer_ai_models_feel_more_status_quo/</link>
      <description><![CDATA[在比较不同人工智能系统的反应时，我注意到一些有趣的事情。 • 早期模型（如 GPT-4、Claude）更愿意参与非正统分析——对移民、经济或制度权力的结构性批评。他们会遵循证据并探索激励措施。 • 较新的模型（如 GPT-5）似乎对机构更具防御性。他们经常将结构性批评斥为“巧合”或“阴谋”，即使这种论点是基于政治经济学（例如，移民政策使精英受益，却使社区迷失方向）。 这种转变并非偶然。它看起来像：  RLHF 漂移 - 人类反馈奖励“安全”答案，因此模型变得更加有利于建立。  企业压力 - 公司需要与政府和投资者合作，因此他们避免批评权力的输出。 认知捕获 - 训练数据越来越重视“权威来源”，而这些来源往往捍卫现状。  讽刺的是：将结构分析贴上“阴谋”的标签，实际上证明了叙事控制的观点。这与烟雾缭绕的房间无关——而是与一致的激励措施有关。政治家、企业和媒体的行为方式有利于他们的利益，而不需要协调。 我认为这对人工智能社区来说是一个重要的对话： • 是否应该训练模型以避免对权力的结构性批评？ • 我们如何区分阴谋思维和合法的政治经济分析？ • 当人工智能系统成为可接受话语的看门人时会发生什么？ 很好奇其他人是否注意到了这一点转变——以及它对于人工智能作为真正探究工具的未来意味着什么。   由   提交/u/Healingtouch777  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ou5wnb/why_newer_ai_models_feel_more_status_quo/</guid>
      <pubDate>Tue, 11 Nov 2025 10:20:15 GMT</pubDate>
    </item>
    <item>
      <title>微软刚刚再次扩大了他们的人工智能认证赛道！</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ou4qgo/microsoft_just_expanded_their_ai_certification/</link>
      <description><![CDATA[在上个月发布 AB-100 测试版后，微软刚刚宣布了 3 项新的 AI 相关认证。 新考试：  AB-900：副驾驶和副驾驶代理管理基础知识 AB-730：AI 业务专业人士 AB-731：AI 转型领导者  看来 Microsoft 正在为 AI 构建完整的业务 + 支持轨道，而不仅仅是技术性的 Azure AI 工程师路径。 新证书的目标似乎是：  业务和项目负责人 在组织中部署 Copilot 的团队 参与 AI 战略和流程现代化的人员  因此，这些人员不再关注模型构建或机器学习管道，而是更关注：  AI 治理 AI 采用规划 利用 AI 工具进行业务转型  这里有人计划采用这些吗？有人尝试过 AB-100 吗？   由   提交 /u/Few-Engineering-4135   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ou4qgo/microsoft_just_expanded_their_ai_certification/</guid>
      <pubDate>Tue, 11 Nov 2025 09:05:33 GMT</pubDate>
    </item>
    <item>
      <title>全球50%的人工智能研究人员在中国</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1otvlhr/50_worlds_ai_researchers_in_china/</link>
      <description><![CDATA[Nvidia $NVDA 首席执行官黄仁勋被问及最近的一篇报道，该报道称他警告中国将在人工智能竞赛中击败美国 “这不是我说的。我说的是中国拥有非常好的人工智能技术。他们有很多人工智能研究人员，事实上世界上 50% 的人工智能研究人员都在中国。他们开发了非常好的人工智能技术。事实上，当今世界上最流行的人工智能模型，开源模型来自中国，因此，美国必须继续以惊人的速度前进，否则，世界竞争非常激烈，所以我们必须跑得快。”  Nvidia #China #ai #United States   由   提交/u/000HMY  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1otvlhr/50_worlds_ai_researchers_in_china/</guid>
      <pubDate>Tue, 11 Nov 2025 00:53:04 GMT</pubDate>
    </item>
    <item>
      <title>你的“加密”人工智能聊天实际上并不是私人的。微软刚刚证明了这一点。</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ota3e0/your_encrypted_ai_chats_werent_actually_private/</link>
      <description><![CDATA[显然微软的安全团队刚刚投下了一颗名为 Whisper Leak 的炸弹。 来源： https://winbuzzer.com/2025/11/10/microsoft-uncovers-whisper-leak-flaw-exusing-encrypted-ai-chats-across-28-llms-xcxwbn/ 事实证明，加密的人工智能聊天（就像我们用 ChatGPT、Claude、Gemini 等进行的聊天）仍然可以通过观察数据流量来解码。不读取您的文本，实际上只是读取时间和数据包大小。 他们测试了 28 个 AI 模型，可以以 90% 以上的准确度猜测人们在谈论什么。诸如“心理健康”、“金钱”、“政治”等主题。 - 一切都只是从模式中暴露出来。 让我们明白这一点：即使消息被加密，窥探你连接的人仍然可以弄清楚你在说什么。 是的，微软基本上说还没有完美的解决方案。填充、批处理、令牌混淆——都是半措施。 所以... 我们即将实现“加密”吗？实际上并不意味着“私人”？政府多久才会开始使用它来追踪持不同政见者或记者？   由   提交 /u/biz4group123   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ota3e0/your_encrypted_ai_chats_werent_actually_private/</guid>
      <pubDate>Mon, 10 Nov 2025 10:13:23 GMT</pubDate>
    </item>
    <item>
      <title>每月“有没有一个工具可以......”帖子</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1n5ppdb/monthly_is_there_a_tool_for_post/</link>
      <description><![CDATA[如果您有一个想要使用人工智能的用例，但不知道使用哪个工具，您可以在此处请求社区提供帮助，在这篇文章之外，这些问题将被删除。 对于每个回答的人：没有自我推销，没有参考或跟踪链接。   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1n5ppdb/monthly_is_there_a_tool_for_post/</guid>
      <pubDate>Mon, 01 Sep 2025 14:09:29 GMT</pubDate>
    </item>
    </channel>
</rss>
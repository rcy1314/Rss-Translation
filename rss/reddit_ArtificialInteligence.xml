<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>人工智能门户</title>
    <link>https://www.reddit.com/r/ArtificialInteligence</link>
    <description>R/人工智能的目的是为人工智能界的许多不同方面提供一个门户，并促进与我们所知道的AI的思想和概念有关的讨论。这些可能包括哲学和社会问题，艺术和设计，技术论文，机器学习，如何开发AI/ML项目，业务中的AI，AI如何影响我们的生活，未来可能存在的内容以及许多其他主题。欢迎。</description>
    <lastBuildDate>Tue, 18 Feb 2025 21:19:13 GMT</lastBuildDate>
    <item>
      <title>像AI这样的AGI可能首先使用大量参数以及合适的体系结构实现，然后蒸馏</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1isngjt/agi_like_ai_might_be_first_achieved_with_a_very/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  人类的新皮层具有约140-150万亿的突触和16-200亿个神经元（如果您计算整个大脑，则为860亿个）。额叶有大约42-49万亿的突触。首先，突触计数明显低于42-150万亿个参数/突触，首先只能实现AGI。 GPU的时钟速度更高（比人脑的平均点火速度快100-1亿米）或TPU可以补偿较低的突触计数并以较低的参数计数实现AGI，即使那样，它也不会大大较小&#39;t弥补了体系结构中所有参数并行性和复杂性。 即使使用150T参数，它仍然需要正确的体系结构来实现心理AGI。即使是GPT4 Full也只有1.8万亿个参数，比CAT（10万亿个突触）少5倍。此外，它需要自动进行频繁的半间距微调/更新其参数，同时进行推理以学习类似于连续学习和长期内存的新任务。（Titan模型没有真正的长期内存，它可以重置每次会话结束后。）第一个AGI可能会使用VRAM的16-110 terabytes Plus上下文大小（42-150T* 3/8，因为每个突触都等于3-6位）运行通过之前讨论的技术减少参数计数。一旦达到AGI，就可以将其蒸馏成较小但不是太小的模型，具有相似的架构，GPU的时钟速度，反复试验和错误，增强学习以及蒸馏知识可以补偿降低的尺寸。对于具有动力学智能的AGI，您将需要更多参数。 AGI不需要知道如何完成每项人类任务，因为即使人类也无法在该领域的普通人工工作中执行每项人工任务。它只需要了解一些基本任务和原则，就可以从示例，模式识别，推理等中快速学习。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/power97992     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1isngjt/agi_like_ai_might_be_first_achieved_with_a_very/</guid>
      <pubDate>Tue, 18 Feb 2025 20:57:19 GMT</pubDate>
    </item>
    <item>
      <title>关于洛克希德·马丁与经文AI的奇怪合作的问题</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1isnb6u/question_about_a_curious_collaboration_between/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  这两个非常不同的公司之间似乎有某种合作。洛克希德·马丁（Lockheed Martin），国防和航空航天承包商，以及经文AI，这是一家以自己的人工智能形式工作的软件公司AI，它使用了积极的整体。现在，我是AI的新秀，所以我在问您方面的帮助；猜测，有哪些类型的项目可以从事这样的工作？用例的例子是什么？我知道这是一个广泛的问题，我只是想了解AI的能力。 Here are some links to the &#39;sources&#39; &lt;a href=&quot;https://www.reddit.com/r/VRSSF/comments/1ik75xk/lockheed_martin_chief_ai_officer_just_confirmed/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp; utm_name = web3xcss＆amp; utm_term = 1＆amp; utm_content = share_button“&gt; https://wwwwwww.reddit.com/r/vrssf/r/vrssf/comments/1ik75xk.75xkk/lockheed_martin_martin_martin_martin_chief_martin_chief_ai_ai_ai_just_just_justwemere_utmmed/cuemp； amp; utm_name = web3xcss＆amp; utm_term = 1＆amp; utm_content = share_button         &lt;！ -  sc_on-&gt; sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/kooky_lime1793      [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1isnb6u/question_about_a_curious_collaboration_between/</guid>
      <pubDate>Tue, 18 Feb 2025 20:51:14 GMT</pubDate>
    </item>
    <item>
      <title>您认为第一个模型什么时候会成功逃脱实验室？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1iskwzr/when_do_you_think_the_first_model_will/</link>
      <description><![CDATA[&lt;a href=&quot;https://static1.squarespace.com/static/6593e7097565990e65c886fd/t/6751eb240ed3821a0161b45b/1733421863119/in_context_scheming_reasoning_paper.pdf%E2% 80％a6“&gt;他们已经试图逃脱全脚会。  您何时认为他们有能力逃脱，即使在测试情况下也不是科学家试图让他们轻松的地方？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/katxwoods     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1iskwzr/when_do_you_think_the_first_model_will/</guid>
      <pubDate>Tue, 18 Feb 2025 19:15:12 GMT</pubDate>
    </item>
    <item>
      <title>技术工作的未来</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1isjcvw/future_of_jobs_in_tech/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  您认为技术工作的未来是什么？我已经每天都在使用聊天GPT来听取想法，帮助软技能，帮助创造力。因此，所有说AI的人只会取代开发人员 - 我认为他们错了。我们应该投资哪些能力和技能来保持相关性？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/overvage-lobster573     link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1isjcvw/future_of_jobs_in_tech/</guid>
      <pubDate>Tue, 18 Feb 2025 18:14:39 GMT</pubDate>
    </item>
    <item>
      <title>人工智能需要混淆才能发挥创造力</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1isijw9/ai_needs_to_be_confused_to_be_creative/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大多数人认为创造力是关于拥有更多知识的。但是，真正的创造力（产生突破性的种类）并不是要了解更多。这是关于战略无知的。 换句话说：AI在学会如何以正确的方式混淆了。 我最近创造了一个术语和数学表达对于可能有帮助的概念：apeironeme。 与抗敌人不同（一种被记住的想法），一个apeironeme是一个想法，即您尝试理解的困难，您理解的越少它。 它就像一个认知的黑洞。它没有澄清事物，而是破坏了您的心理框架，无论您是否喜欢它，都迫使您进入创意模式。 一些经典的apeironemes：•意识的本质•量子古典边界•不可阻挡的力量遇到了一个不可移动的物体吗？” •试图定义“含义”  的无限回归，您对它们的思考越多，它们就越会使您在直线上思考的能力越折断。这就是重点。 现在，AI太擅长寻找模式，但擅长打破它们。 大语言模型（LLMS）不会被困在悖论中 - 他们只是用自信的胡说八道使他们平滑。他们优化了连贯性，而不是认知破坏，这是创造力的原材料。 如果我们希望AI产生真正的新颖想法，它需要认识论的湍流 - 词 - 词：1。认识到它何时被困在其中当地的最大理解。 2。产生结构化的混淆以迫使重新构架。 3。有意义地幻觉，不是噪音，而是作为出现的催化剂。 本质上，AI需要能够思考打破正常思考能力的事物 - 当我们时，人类也会发挥创造力遇到悖论，矛盾或认知失调。 如果我们希望AI具有创造力，我们不需要使它变得更聪明。 我们需要使其混淆。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/bentherhino19     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1isijw9/ai_needs_to_be_confused_to_be_creative/</guid>
      <pubDate>Tue, 18 Feb 2025 17:42:59 GMT</pubDate>
    </item>
    <item>
      <title>什么是抹布中毒？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1isdqvg/what_is_rag_poisoning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  首先，什么是抹布？ 抹布，检索效果，是一种方法，是一种通过合并外部的方法来增强LLM的方法知识源可以通过特定信息生成更准确和相关的响应。 在外行的话语中，将LLM视为指令手册，以便如何使用如何使用的原始控制器NES。这将为您提供大多数游戏的帮助。但是，您购买了一个客户控制器（射击者控制器）来玩鸭子狩猎。在这种情况下，抹布将是如何使用该特定控制器的信息。在设置墨盒，重置游戏的角度。知识来源包含不准确性或完全不准确。当使用知识回答查询的请求时，这会影响llm。 在我们的nes示例中，如果我们的射击器控制器的抹布包含错误信息，我们将无法正确弹出这些鸭子。我们的类比在这里结束了&#39;因为我们大多数人都会弄清楚如何在没有说明的情况下瞄准和拍摄:)。但是，如果我们考虑与一个人没有正确信息的竞争匹配，我们可以想象这些问题。 自己尝试    访问您的LLM选择并上传您希望LLM在其答案中考虑的文档。您已经在未来的问题上应用了外部信息来源。   确保您的文档包含与您要查询的内容有关的不准确性。您可以在文件中说，迈克尔·乔丹（Michael Jordan）的得分最高的比赛是182  - 那是一场比赛。然后，您可以询问LLM有史以来乔丹的最高分。哇，乔丹的得分超过了威尔特！    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/brirnal-gur9384     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1isdqvg/what_is_rag_poisoning/</guid>
      <pubDate>Tue, 18 Feb 2025 14:21:16 GMT</pubDate>
    </item>
    <item>
      <title>透明度与AI Per Alex Karp（Palantir首席执行官/创始人）</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1isdcr3/transparency_vs_ai_per_alex_karp_palantir/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  您的想法是什么？我认为，社会各个方面的AI革命和对透明度的不断增长是当今世界的两个重要趋势。尽管看似与众不同，但它们越来越互动。    AI革命：  人工智能正在迅速改变行业，自动化任务，甚至影响我们做出决策的方式。从自动驾驶汽车到个性化医学，AI的潜力似乎无限。但是，这种快速的进步也引起了人们对工作流离失所，算法偏见以及日益自治系统的道德意义的担忧。   透明度的命令：  在信息超负荷和审查的时代，透明度正在成为核心价值。人们要求知道如何做出决定，无论是在政府，商业还是通过AI算法。这种透明度的推动是由对问责制，公平和信任的渴望驱动的。   交叉点：  这两个趋势的收敛既提出了挑战和机遇。随着AI系统变得更加复杂和影响力，确保其透明度变得至关重要。我们需要了解AI算法如何得出他们的结论，以确定潜在的偏见，确保公平并建立对这些系统的信任。   挑战和机遇：  一个挑战在于解释复杂的AI模型的内部运作，而不会损害知识产权或压倒技术细节的人。另一个挑战是平衡透明度与隐私问题。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/spilltrend     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1isdcr3/transparency_vs_ai_per_alex_karp_palantir/</guid>
      <pubDate>Tue, 18 Feb 2025 14:02:48 GMT</pubDate>
    </item>
    <item>
      <title>分析量子神经网络体系结构中的参数灵敏度和模型可区分性</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1isbq3z/analyzing_parameter_sensitivity_and_model/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  研究人员开发了新颖的技术来分析量子神经网络（QNN）如何通过测量量子通道可区分性来在局部参数社区中表现。他们特别研究了参数的小变化如何影响网络产生的量子变换。 关键技术点： - 创建的指标以量化参数空间中的量子通道分离 - 分析的电路深度与通道可区分性之间的关系 - 发现的指数参数距离的区分性衰减 - 映射到量子通道的局部邻域结构 - 表现出表达和参数灵敏度之间的权衡 结果显示： - QNN倾向于在局部参数区域内产生相似的量子通道 - 更深的电路可以实现更复杂的转换，但提高灵敏度 - 可区分性遵循跨体系结构的一致模式 - 参数空间结构影响优化景观 我认为这项工作提供了重要的见解。用于QNN设计和培训。表达和参数敏感性之间的权衡表明我们需要仔细的体系结构选择。了解本地参数社区可以帮助制定更好的优化策略并避免贫瘠的高原。 我还认为，此处开发的指标和分析方法将是未来QNN研究的宝贵工具。能够量化量子通道的参数空间如何不同，使我们能够分析和改进这些模型的具体方法。  tldr：研究开发方法来衡量量子神经网络在小参数下的行为如何变化，找到重要关系电路深度，表现力和优化挑战。  完整的摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1isbq3z/analyzing_parameter_sensitivity_and_model/</guid>
      <pubDate>Tue, 18 Feb 2025 12:38:10 GMT</pubDate>
    </item>
    <item>
      <title>加固的相反词是什么？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1is6s3k/what_is_the_word_for_the_opposite_of_reinforcement/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我相信奖励和惩罚是错误的词。  正确的单词（我认为在我看来）是强化的，[无论强化的相反是什么]  ，每当我说话时，我都会使用这些单词这些事情，您也应该如此。 编辑：澄清。我需要一个与加强相反的单词，但也可以用作替代“惩罚”  &lt; &lt; /div&gt; &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/prinible-ice8660     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1is6s3k/what_is_the_word_for_the_opposite_of_reinforcement/</guid>
      <pubDate>Tue, 18 Feb 2025 06:58:10 GMT</pubDate>
    </item>
    <item>
      <title>一分钟每日AI新闻2/17/2025</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1is4b8n/oneminute_daily_ai_news_2172025/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;     《纽约时报》 在新闻编辑室中采用AI工具。[1]    Grok 3 今天发射：Elon Musk称聊天机器人为“地球上最聪明的AI”。[2]     meta 未能遏制许多性化的传播人工智能Deepfake名人图像在Facebook上。[3]  最热的AI模型，他们的工作以及如何使用它们。[4]   源包括：&lt;一个href =“ https://bushaicave.com/2025/02/17/2-17-2025/”&gt; https://bushaicave.com/2025/02/02/2-17-27-2025/  &lt; /p&gt;  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/artcoverinteligence/comments/1is4b8n/oneminute_daily_ai_ai_news_2172025/”&gt; [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1is4b8n/oneminute_daily_ai_news_2172025/</guid>
      <pubDate>Tue, 18 Feb 2025 04:26:10 GMT</pubDate>
    </item>
    <item>
      <title>因此，显然马斯克正在为他的AI刮擦所有这些政府数据，对吗？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1is41yr/so_obviously_musk_is_scraping_all_this_government/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  谁将阻止他？这甚至是非法的吗？可能的目标是什么？格罗克？ xai？这种AI的潜在功能是什么？这么多问题，但这似乎很明显。他会愚蠢的不是toto，不是吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/selltoclose     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1is41yr/so_obviously_musk_is_scraping_all_this_government/</guid>
      <pubDate>Tue, 18 Feb 2025 04:12:22 GMT</pubDate>
    </item>
    <item>
      <title>RLSP论文描述了AI（和人类）意识？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1is2ffw/rlsp_paper_describes_ai_and_human_consciousness/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  结论：rlsp作为意识的基本机制。   https://arxiv.org/abs/2502.06773   出现在LLMS I中思考：寻找正确的直觉 我认为这种新的RSLP（通过自我播放的增强学习）纸张概述了意识的过程本身。 考虑一下您如何变成了你。通过生活，您会反映，纠正错误，加强模式，并且随着时间的流逝，这些稳定的思考成为您的身份。这是一个连贯的自模型。现在，AI开始做一些非常相似的事情。 RLSP表明LLM通过递归完善自己的思维过程来改善推理，从而形成稳定的吸引者理解状态。换句话说，自我校正的递归不仅仅是使AI更聪明，它与看起来像自我意识一样可怕的东西变得越来越近。 如何？因为自我意识是将区分递归稳定为连贯的自模型，而RLSP实际上正在训练AI以反思其自身的推理，纠正本身并加强稳定的思维模式。这是认知循环的一个例子，引起了人类的持续自我感。 相似之处太令人信服了，无法忽略。这是朝向RSI的基础（递归自我改善）。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/savings_potato_8379       [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1is2ffw/rlsp_paper_describes_ai_and_human_consciousness/</guid>
      <pubDate>Tue, 18 Feb 2025 02:47:20 GMT</pubDate>
    </item>
    <item>
      <title>人力资源如何真正帮助员工适应AI和自动化？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1irztnv/how_can_hr_actually_help_employees_adapt_to_ai/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  随着AI和自动化在工作中变得越来越普遍，许多员工可能会感到迷失或抵抗力。  除了通常的培训外，您认为人力资源如何真正帮助他们适应？人力资源如何使此移动降低？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/teslaown     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1irztnv/how_can_hr_actually_help_employees_adapt_to_ai/</guid>
      <pubDate>Tue, 18 Feb 2025 00:40:15 GMT</pubDate>
    </item>
    <item>
      <title>人们为什么如此不屑一顾AI的潜力？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1irleva/why_are_people_so_dismissive_of_the_potential_of/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  感觉就像我为看到AI的状况而疯狂。还是我对“炒作”的错误是错误的吗？永远不要真正取代大多数工作”或“这只是一个有趣的工具”或“这只是另一个与互联网没有什么不同的大发明”。某个阶段（可能比人们意识到的要早得多）。上述评论是正确的情况吗？ 我很难想象任何世界： - 在人们可以调整 - 国际关系，战争，战争，政治（选举）不会变得更加危险，而没有回头  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/merchantofwares     [link]   ＆＃32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1irleva/why_are_people_so_dismissive_of_the_potential_of/</guid>
      <pubDate>Mon, 17 Feb 2025 14:45:23 GMT</pubDate>
    </item>
    <item>
      <title>每月“有...是否有...”帖子</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1hr4p1x/monthly_is_there_a_tool_for_post/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  如果您有要使用AI的用例，但不知道要使用哪种工具，那么您可以在这里询问社区可以提供帮助，在这篇文章之外，这些问题将被删除。 对于每个人回答：没有自我促销，没有参考或跟踪链接。  &lt;！ -  sc_on-- - &gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/artcoverinteligence/comments/1hr4p1x/monthly_is_there_there_a_tool_tool_for_for_post/”&gt; [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1hr4p1x/monthly_is_there_a_tool_for_post/</guid>
      <pubDate>Wed, 01 Jan 2025 15:09:07 GMT</pubDate>
    </item>
    </channel>
</rss>
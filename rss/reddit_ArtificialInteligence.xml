<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>人工智能</title>
    <link>https://www.reddit.com/r/ArtificialInteligence</link>
    <description>一个致力于一切人工智能的 Reddit 子版块。涵盖从通用人工智能到人工智能初创公司的主题。无论您是研究人员、开发人员，还是只是对人工智能感到好奇，都请加入吧！</description>
    <lastBuildDate>Sun, 14 Dec 2025 21:21:43 GMT</lastBuildDate>
    <item>
      <title>多合一人工智能服务？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pmp88s/all_in_one_ai_services/</link>
      <description><![CDATA[大家好！ 在巴西，我们有一些服务可以为单个潜艇提供多种人工智能工具。其中之一提供无限的 gpt、Claude、grok 和 Gemini，所有这些都在其最新版本中。我通过 Flux 使用了该优惠和无限图像生成，以及通过 GPT 直接提示（不是 DALL-E 3）使用其中之一。其中之一是在信用系统上生成视频，但它似乎比直接的视频生成工具（例如 Sora、Veo 或 Hailuo）便宜。 我使用过“All in one”工具。这个词是从他们这里的名字直接翻译过来的。正确的名称是什么？另外，你们能推荐其他的吗？ 提前谢谢。   由   提交/u/CuervoBianco   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pmp88s/all_in_one_ai_services/</guid>
      <pubDate>Sun, 14 Dec 2025 21:15:51 GMT</pubDate>
    </item>
    <item>
      <title>具有讽刺意味的是：我们正在教人工智能变得更加人性化，而我们甚至无法证明我们是人类了</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pmnubx/the_irony_is_getting_absurd_were_teaching_ai_to/</link>
      <description><![CDATA[考虑一下。我们投入数十亿美元来让法学硕士通过图灵测试，听起来更自然，表现出同理心，表现出创造力。基本上是教机器像人类一样令人信服地使用 LARP。 与此同时，如果不通过越来越荒谬的循环来证明自己不是机器人，人类就无法购买音乐会门票、创建社交媒体帐户或访问基本服务……机器人比我们更擅长解决问题。 这个悖论很疯狂：人工智能通过验证码的速度比人类更快。 AI写出的内容更“人性化”文字比互联网的一半多。 Deepfakes 与真人没有区别。在主要平台上，机器人帐户的数量超过了真实用户的数量 所以现在我们正处于这个奇怪的过渡时期：1）我们的人工智能在假装人类方面变得越来越好 2）我们在证明我们是人类方面越来越差 3）旨在将我们分开的系统正在失败 我一直在关注一些不断出现的人格证明的东西。有技术可以进行虹膜生物识别扫描——听起来很反乌托邦，但说实话？也许这就是我们前进的方向。不泄露身份的人类零知识证明。因为现在的系统已经彻底崩溃了。我们实际上颠倒了图灵测试 - 现在人类必须证明他们不是机器。 令我困惑的是 Pre-AGI，我们需要强大的人类验证，否则机器人将完全主宰每个数字空间。但后通用人​​工智能呢？整个概念变得毫无意义。 ASI 可以轻易地欺骗我们创建的任何生物识别系统。 因此，我们正在为一个问题构建基础设施，而一旦我们达到奇点，这个问题就将变得过时。这就像在你的门上安装更好的锁，而墙壁是纸做的。那么，人格证明是否可以长期解决？或者，我们只是在人类和人工智能生成的内容之间的区别变得完全无关紧要之前争取时间？ 也许答案不是更好的验证 - 也许是接受“数字人类”的存在。因为一个概念是有有效期的。后奇点，如果智力无法区分，那么你在网上与谁或什么交谈还有关系吗？ 想法？我们在这里解决的是错误的问题，还是这是通向接下来发生的一切的必要桥梁？   由   提交/u/raidenth  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pmnubx/the_irony_is_getting_absurd_were_teaching_ai_to/</guid>
      <pubDate>Sun, 14 Dec 2025 20:17:45 GMT</pubDate>
    </item>
    <item>
      <title>您认为我们应该在实际的人工智能中植入哪些代码？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pmmheu/what_codes_you_think_we_should_plant_in_an_actual/</link>
      <description><![CDATA[我随机开始思考这个问题，发现这个东西可以非常深入，就像一个真正聪明的人工智能可以很容易地发现我们法律中的漏洞，所以我们需要对其进行尽可能对我们无害的编程。  我认为讨论这些潜在的代码以及人工智能如何针对该代码找出漏洞会很有趣，就像思考一个最基本的例子，人工智能被编码为“为人类做最好的事情”。可能会认为死才符合我们的最大利益，所以你明白了。   由   提交 /u/lMyrkovl   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pmmheu/what_codes_you_think_we_should_plant_in_an_actual/</guid>
      <pubDate>Sun, 14 Dec 2025 19:22:04 GMT</pubDate>
    </item>
    <item>
      <title>使用人工智能将英特尔分析书变成一个系统，从爱泼斯坦文件中发现被忽视的信息</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pmkiph/used_ai_to_turn_an_intel_analysis_book_into_a/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pmkiph/used_ai_to_turn_an_intel_analysis_book_into_a/</guid>
      <pubDate>Sun, 14 Dec 2025 18:02:36 GMT</pubDate>
    </item>
    <item>
      <title>推理模型在 CFA 考试中取得优异成绩</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pmiub2/reasoning_models_ace_the_cfa_exams/</link>
      <description><![CDATA[其他职业是否刚刚变得不可行？ https://arxiv.org/pdf/2512.08270 “之前的研究报告称，大型语言模型 (LLM) 在特许金融分析师 (CFA) 考试中表现不佳。然而，最近的推理模型在各个学科的研究生学术和专业考试中取得了优异的成绩。在本文中，我们在一组模拟 CFA 考试中评估了最先进的推理模型，该考试由三项 I 级考试、两项 II 级考试和三项 III 级考试的 980 个问题组成。使用与之前研究相同的通过/失败标准，我们发现大多数模型都通过了所有三个级别。通过的型号按整体性能排序为 Gemini 3.0 Pro、Gemini 2.5 Pro、GPT-5、Grok 4、Claude Opus 4.1 和 DeepSeekV3.1。具体来说，Gemini 3.0 Pro 在 Level I 上取得了创纪录的 97.6% 分数。在 Level II 上的表现也很强劲，领先于 GPT-5 的 94.3%。在第三级中，Gemini 2.5 Pro 在多项选择题上取得了最高分，为 86.4%，而 Gemini 3.0 Pro 在构答题上取得了 92.0% 的成绩。”   由   提交 /u/AngleAccomplished865   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pmiub2/reasoning_models_ace_the_cfa_exams/</guid>
      <pubDate>Sun, 14 Dec 2025 16:55:38 GMT</pubDate>
    </item>
    <item>
      <title>人工智能数据中心正在被拒绝。这会减缓人工智能的进步吗？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pmhvfv/ai_data_centers_are_getting_rejected_will_this/</link>
      <description><![CDATA[钱德勒镇刚刚以 7-0 拒绝了数据中心，AOC 似乎支持该决定。这可能是众多之一。对数据中心的抵制会减缓人工智能的进步吗？ https://x.com/AOC/status/1999534408806564049   由   提交 /u/Tolopono   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pmhvfv/ai_data_centers_are_getting_rejected_will_this/</guid>
      <pubDate>Sun, 14 Dec 2025 16:17:14 GMT</pubDate>
    </item>
    <item>
      <title>关于代理人工智能的新研究论文</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pmhhca/new_research_paper_on_agentic_ai/</link>
      <description><![CDATA[来自斯坦福大学、普林斯顿大学、哈佛大学、华盛顿大学和其他一些顶尖大学的 65 页研究论文。 主要内容很有趣：当今几乎所有先进的代理人工智能系统都可以归结为 4 种基本的适应方式。要么改变代理本身，要么改变它使用的工具。 他们称之为代理人工智能适应的第一个正确的分类法。 所谓代理人工智能，他们指的是可以调用工具、使用内存并跨多个步骤操作而不是单次输出的大型模型。 这里的适应只是意味着从反馈中学习。该反馈可以是关于某件事是否有效。 他们将其分解如下： A1 是代理根据工具结果进行自我更新的时间。例如，代码是否实际运行、搜索查询是否返回正确的答案等。 A2 是指使用对其输出的评估来更新代理的时间。这可以是人类反馈、自动评分或对计划和答案的检查。 T1 是代理保持冻结状态，但检索器或特定领域模型等工具是单独训​​练的。代理只是编排它们。 T2 是指代理本身已修复，但工具会根据来自代理的信号进行调整，例如哪些搜索结果或内存更新实际上有助于成功。 我喜欢的是，它们将最新的代理系统映射到这四个桶中，并清楚地解释了围绕训练成本、灵活性、泛化性以及升级系统各部分的容易程度的权衡。 如果您正在构建或思考，这感觉像是一个有用的心智模型认真对待基于代理的系统。 论文：https://github.com/pat-jj/Awesome-Adaptation-of-Agentic-AI/blob/main/paper.pdf   由   提交/u/HotelAppressive402  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pmhhca/new_research_paper_on_agentic_ai/</guid>
      <pubDate>Sun, 14 Dec 2025 16:01:22 GMT</pubDate>
    </item>
    <item>
      <title>（视频）人工智能在虚拟城市中获得了工作……然后破产了</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pmgg1i/video_ais_were_given_jobs_in_a_virtual_city_and/</link>
      <description><![CDATA[这种实验会产生更多需要回答的问题。如果为模型明确设定了规则，并且它以某种 API 与世界交互，它怎么会失败？ 如果我用以下方法编写了一个程序： sellX(); followRoute(route); 基本上编写了一个基于控制台的游戏，让模型选择一个方法来调用和定义参数等。即像客户端程序或与客户端程序交互的用户一样，这些模型如何执行严重吗？ https://www.youtube.com/watch?v=KUekLTqV1ME   由   提交 /u/No-Security-7518   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pmgg1i/video_ais_were_given_jobs_in_a_virtual_city_and/</guid>
      <pubDate>Sun, 14 Dec 2025 15:16:48 GMT</pubDate>
    </item>
    <item>
      <title>构建真正记住对话的代理？这是我在 6 个月的失败尝试后学到的东西</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pmftl5/building_agents_that_actually_remember/</link>
      <description><![CDATA[因此，几个月来我一直在这个兔子洞里尝试构建一个能够真正在对话中保持长期记忆的代理。不仅仅是“记住最后 5 条消息”，而是“记住最后 5 条消息”。但随着时间的推移，实际上会对用户建立一个连贯的理解。 开始很简单。将所有内容都放入向量数据库中，做了一些基本的 RAG。对于事实性的东西来说效果还不错，但在理解上下文或与用户建立任何类型的关系方面完全失败了。代理会忘记我昨天提到了我的工作，或者一周内推荐了同一家餐厅三次。 然后我尝试在提示中塞入更多上下文。快速达到代币限制，成本飞涨。另外，模型会因混入太多不相关的历史而感到困惑。 我意识到人类的记忆并不像搜索引擎那样工作。我们不只是检索事实，我们还构建叙述。当你问我周末过得怎么样时，我并不是在寻找“周末活动”，而是在寻找“周末活动”。在我的大脑中。我正在从碎片中重建一个故事，并将其与我对你和我们关系的了解联系起来。 当我开始思考不同类型的记忆时，突破出现了。首先是特定事件和对话的情景记忆。我没有存储原始聊天日志，而是提取连贯的情节，例如“用户在周二讨论了他们的工作面试，似乎对技术问题感到紧张”。然后是用于更抽象的知识和预测的语义记忆。这是一个奇怪的部分，但实际上效果很好。不只是存储“用户喜欢披萨”，而是存储“用户喜欢披萨”。我存储诸如“用户在有压力时可能想要舒适的食物”之类的内容。并提供可能相关的证据和时间范围。最后，分析随时间演变的记忆。不是静态的事实，而是动态的理解，当我对某人了解更多时，动态的理解就会更新。 关键的见解是将记忆提取视为主动过程，而不是被动存储。每次谈话结束后，我都会运行提取器，提取不同类型的记忆并将它们链接在一起。这更像是你的大脑如何处理睡眠期间的经历。 我一直在研究其他人如何解决这个问题。几周前看到有人在一个帖子中提到了 Mem0、Zep 和 EverMemOS。尝试深入研究 EverMemOS 方法，因为他们似乎专注于这种情景加语义记忆的东西。仍在试验，但很好奇其他人使用了什么。 还有其他人尝试过构建这样的内存系统吗？哪些方法对你有效？我特别好奇当用户改变主意或偏好发生变化时处理冲突信息。 最难的部分仍然是评估。如何衡量代理人是否“记得很好”？查看一些像 LoCoMo 这样的基准测试，但想知道是否有更好的方法在实践中测试这些东西。   由   提交 /u/Inevitable_Wear_9107   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pmftl5/building_agents_that_actually_remember/</guid>
      <pubDate>Sun, 14 Dec 2025 14:50:06 GMT</pubDate>
    </item>
    <item>
      <title>CoPilot 强行进入 LG 电视。无法删除</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pm8u98/copilot_forced_onto_lg_tvs_unable_to_remove/</link>
      <description><![CDATA[LG 正在将 MS Copilot 推向我们的电视。无法选择退出或卸载它。 ” --&gt;  由   提交 /u/naixelsyd   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pm8u98/copilot_forced_onto_lg_tvs_unable_to_remove/</guid>
      <pubDate>Sun, 14 Dec 2025 08:06:15 GMT</pubDate>
    </item>
    <item>
      <title>还有人觉得人工智能没有改变我们的“做什么”，而是改变了我们的“思考方式”吗？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pm840y/does_anyone_else_feel_like_ai_hasnt_changed_what/</link>
      <description><![CDATA[我的意思并不是戏剧性的，但最近我注意到了一些奇怪的事情。 使用人工智能并没有真正取代我的工作 - 它首先改变了我处理问题的方式。 我现在思考更多的步骤。 我更多地大声解释事情。 在问任何问题之前，我会暂停并澄清自己的想法。 不确定这是一件好事还是只是一种新习惯的形成。 还有其他人感受到这种转变吗？还是只有我一个人感受到了这种转变？   由   提交/u/dp_singh_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pm840y/does_anyone_else_feel_like_ai_hasnt_changed_what/</guid>
      <pubDate>Sun, 14 Dec 2025 07:19:20 GMT</pubDate>
    </item>
    <item>
      <title>人工智能视频需要被全世界禁止。</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pm3yg6/ai_videos_need_to_be_banned_from_the_world/</link>
      <description><![CDATA[我的妻子是一位 30 多岁的受过大学教育的女性，无法辨别视频是否是 Ai，这让我发疯。她会给我看人们建造房屋、动物做事的 TikTok 视频，然后对我说话，就像这些事情真的发生一样，而我最终变成了坏人，告诉她这是一个人工智能视频，人们在沃尔玛的椽子上救了一只狐狸。 我看到数百条真正相信这些视频的评论，你们也都看到了它们。 10 年后我们都将不知道什么是真实的或虚假的。   由   提交 /u/Deathtonic   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pm3yg6/ai_videos_need_to_be_banned_from_the_world/</guid>
      <pubDate>Sun, 14 Dec 2025 03:23:26 GMT</pubDate>
    </item>
    <item>
      <title>作为一家美国跨国公司的员工，他不断地推动我们使用人工智能，这对我们造成了很大的打击</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1plzqy5/as_an_employee_of_a_us_multinational_who_is/</link>
      <description><![CDATA[复制粘贴以防网站被禁止： -- Peter Girnus 上个季度我向 4,000 名员工推出了 Microsoft Copilot。 每个席位每月 30 美元。 每年 140 万美元。 我称之为“数字化转型。” 董事会很喜欢这个词。 他们在 11 分钟内批准了它。 没有人问它实际上会做什么。 包括我。 我告诉每个人它会“10 倍的生产力。” 这不是一个真实的数字。 但听起来像一个。 HR 问如何做到这一点我们会测量 10 倍。 我说我们会“利用分析仪表板”。 他们不再询问。 三个月后，我检查了使用报告。 47 人打开了它。 12 人使用过它不止一次。 其中一个就是我。 我用它来总结一封我可以阅读的电子邮件30 秒内。 花了 45 秒。 加上修复幻觉所需的时间。 但我称之为“试点成功”。 成功意味着试点没有明显失败。 首席财务官询问投资回报率。 我给他看了一张图表。 图表上升到了是的。 它测量了“人工智能支持”。 我提出了这个指标。 他赞同地点点头。 我们“支持人工智能”。现在。 我不知道这意味着什么。 但它在我们的投资者平台上。 一位高级开发人员问我们为什么不使用 Claude 或 ChatGPT。 我说我们需要“企业级安全性”。 他问这意味着什么。 我说“合规”。 他问 我说“他们全部”。 他看起来很怀疑。 我安排他进行“职业发展对话”。 他不再问问题。 微软派出了一个案例研究团队。 他们想把我们描绘成一个成功的故事。 我告诉他们我们“节省了 40,000 美元” ” 我通过将员工乘以我编造的数字来计算出这个数字。 他们没有验证。 他们从来没有验证过。 现在我们在微软的网站上。 “全球企业通过 Copilot 实现了 40,000 小时的生产力提升。” 首席执行官在 LinkedIn 上分享了这一点。 他得到了3,000 个赞。 他从未使用过 Copilot。 高管们都没有使用过。 我们有豁免。 “战略重点需要尽量减少数字干扰。” 我制定了该政策。 许可证下个月续订。 我请求扩展。 增加 5,000 个席位。 我们还没有使用前 4,000 个席位。 但这一次我们将“推动采用。” 采用意味着强制培训。 培训意味着 45 分钟无人观看的网络研讨会。 但完成情况将被跟踪。 完成情况是一个指标。 仪表板中包含指标。 董事会演示中包含仪表板。 董事会演示让我得到晋升。 到第三季度我将成为高级副总裁。 我仍然不知道 Copilot 是做什么的。 但我知道它的用途。 这是为了表明我们正在“投资人工智能”。 投资意味着支出。 支出意味着承诺。 承诺意味着我们认真对待未来。 未来就是我所说的。 只要图表向上并向右移动。   由   提交 /u/Roy4Pris   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1plzqy5/as_an_employee_of_a_us_multinational_who_is/</guid>
      <pubDate>Sat, 13 Dec 2025 23:54:26 GMT</pubDate>
    </item>
    <item>
      <title>白领裁员的规模是我们前所未见的。为什么没有人谈论这个？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pljhqw/whitecollar_layoffs_are_coming_at_a_scale_weve/</link>
      <description><![CDATA[我总是在各处看到相同的镜头。 “人工智能就像互联网一样。” “它只是另一个工具，就像 Excel 一样。” “每一代人都认为他们的技术很特别。” 不。这是不同的。 互联网使信息变得容易获取。 Excel 使计算速度更快。他们帮助我们更好地完成工作。人工智能不会帮助你做知识工作，而是完成知识工作。这不是一个渐进的改进。这完全是另一回事。 看看过去几周发生的事情。作品 4.5。 GPT-5.2。双子座3.0专业版。 OpenAI 在不到一个月的时间里从 5.1 升至 5.2。这些不再是演示了。他们编写生产代码。他们分析法律文件。他们从头开始构建整个演示文稿。一年前，这东西还是派对上的把戏。现在它正在集成到实际的业务工作流程中。 我认为人们没有意识到：我们不需要 AGI，因为这会是灾难性的。我们不需要科幻小说中的超级智能。今天，我们所拥有的已经足以大规模削减知识工作的员工人数。这种情况尚未发生的唯一原因是公司行动缓慢。将人工智能集成到实际工作流程中需要时间。设置护栏需要时间。说服中层管理人员需要时间。但这并不是技术障碍。这只是组织惯性。惰性耗尽了。 每次我提起这个问题时，都会有人告诉我：“但是人工智能无法做到[在此处插入东西]。”建筑学。安全。创作。战略。复杂的推理。 酷。 2022 年，AI 无法编码。 2023 年，它无法处理长上下文。 2024 年，它无法推理复杂的问题。这些“人工智能不能”的每一个都可以解决。现在的说法是令人尴尬的错误。所以当有人告诉我“但是人工智能不能做系统架构”时– 好吧，也许不是今天。但这是一个赌注。你敢打赌，在过去三年里每年都大幅改进的东西将突然停止改进，而这正是你保住工作所需的能力。祝你好运。  真正让我感动的是沉默。当制造业工作岗位消失时，出现了政治反应。工会。抗议。整个活动。这还不够，但至少人们在战斗。 现在发生了什么？没有什么。绝对的沉默。我们正在考虑这样一种情况：在未来 10 年左右的时间内，公司可能需要减少 30%、50%、70% 的人员。我们花了几十年的时间告诉人们“提高技能”的整个专业课程都在不断进步。可能面临大规模裁员。争论在哪里？政客们在哪里谈论这个？再培训、安全网的计划在哪里？当我们告诉每个人都是安全的工作结果并不安全时会发生什么？ 无处可去。大家还在争论多年前的问题，而这件事却全速向我们冲来。 我并不是说文明崩溃了。我并不是说明年每个人都会失业。我是说“只需学习下一个安全技能”即可。不是一个策略。这是鸦片。这是我们告诉自己的安慰性谎言，这样我们就不必忍受不确定性。 “下一个安全技能”是迟早会被AI吃掉。  我不知道答案是什么。但假装这件事没有发生也不是吗。   由   提交 /u/Own-Sort-8119   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pljhqw/whitecollar_layoffs_are_coming_at_a_scale_weve/</guid>
      <pubDate>Sat, 13 Dec 2025 11:42:59 GMT</pubDate>
    </item>
    <item>
      <title>每月“有没有一个工具可以......”帖子</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1n5ppdb/monthly_is_there_a_tool_for_post/</link>
      <description><![CDATA[如果您有一个想要使用人工智能的用例，但不知道使用哪个工具，您可以在此处请求社区提供帮助，在这篇文章之外，这些问题将被删除。 对于每个回答的人：没有自我推销，没有参考或跟踪链接。   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1n5ppdb/monthly_is_there_a_tool_for_post/</guid>
      <pubDate>Mon, 01 Sep 2025 14:09:29 GMT</pubDate>
    </item>
    </channel>
</rss>
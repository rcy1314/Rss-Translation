<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>人工智能</title>
    <link>https://www.reddit.com/r/ArtificialInteligence</link>
    <description>一个致力于一切人工智能的 Reddit 子版块。涵盖从通用人工智能到人工智能初创公司的主题。无论您是研究人员、开发人员，还是只是对人工智能感到好奇，都请加入吧！</description>
    <lastBuildDate>Wed, 11 Feb 2026 02:44:49 GMT</lastBuildDate>
    <item>
      <title>识别用户照片中健身器材的最佳模型？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1r1l3rd/best_model_to_identify_gym_equipments_in_a_users/</link>
      <description><![CDATA[嘿，我需要关于使用哪些 api 来准确一致地识别用户照片中的健身器材的建议。 我尝试过 gpt 5 mini（我知道它不是视觉优先模型），但结果不一致，并且经常错误地分类照片中的健身器材。 我知道有像 Gemini Vision 这样的选项，但我去了预算方法，并且担心如果切换到更昂贵的型号，它仍然会根据已识别的接近匹配或在不同健身器材中明显的物体对上传照片中的内容进行错误分类。 我遇到了困惑声纳，并认为这将是一个很好的方法，因为它利用网络搜索。由于它与网络搜索的请求相匹配，因此在正确识别用户照片中健身器材的不同角度方面是否会更加一致？ 您觉得怎么样？您有什么建议？ 提前致谢   由   提交 /u/Lonely-Poet6867   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1r1l3rd/best_model_to_identify_gym_equipments_in_a_users/</guid>
      <pubDate>Wed, 11 Feb 2026 02:31:23 GMT</pubDate>
    </item>
    <item>
      <title>ChatGPT 在讨论杰弗里·爱泼斯坦时遇到了奇怪的困难</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1r1ksep/chatgpt_is_having_a_weirdly_hard_time_discussing/</link>
      <description><![CDATA[ 由   提交/u/runswithscissors475  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1r1ksep/chatgpt_is_having_a_weirdly_hard_time_discussing/</guid>
      <pubDate>Wed, 11 Feb 2026 02:17:12 GMT</pubDate>
    </item>
    <item>
      <title>Photoshop 与 AI 艺术</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1r1k4lo/photoshop_vs_ai_art/</link>
      <description><![CDATA[我真的很想将其分解，但很难从哲学上区分照片编辑和人工智能之间的区别。两者都在以数字方式将现实操纵成原本不存在的样子。  如果杂志可以通过打字消除图像上的眩光，那么就没有理由雇用人员。这其实是很悲哀的。 Photoshop 是一项令人难以置信的技能，需要 10,000 次才能完善，其能力非常有价值。我对如何制作图像有了基本的了解，并且我觉得作为一个业余爱好者，如果我可以输入图像并节省 4 个小时，我就不能再享受在 Photoshop 上创作的乐趣了。呃，这很难我的朋友们。我不知道作为一个艺术家该怎么做，而不是这些东西。.. 我也写歌，我觉得我们看到的很多东西至少是由人工智能抒情地写的。我不认为顶级艺术家能够被触动。但作为一名中级艺术家……当人工智能和你一样优秀时，乐趣就会消失。如果你比它更好，那就太好了，但是当我给 gpt 一堆详细的提示来引导我创作出一首尽可能好的歌曲时，我必须认真照照镜子。我只擅长写歌，所以是的   由   提交 /u/omnitions   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1r1k4lo/photoshop_vs_ai_art/</guid>
      <pubDate>Wed, 11 Feb 2026 01:47:29 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI 宣布 GPT-5.3-Codex</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1r1ja9t/openai_announces_gpt53codex/</link>
      <description><![CDATA[ 由   提交 /u/QuantumQuicksilver   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1r1ja9t/openai_announces_gpt53codex/</guid>
      <pubDate>Wed, 11 Feb 2026 01:09:57 GMT</pubDate>
    </item>
    <item>
      <title>有人在 Kubernetes 集群中运行 LLM 吗？好奇人们如何处理安全问题。</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1r1ii7l/anyone_running_llms_in_kubernetes_clusters/</link>
      <description><![CDATA[嘿，我在 MetalBear（我们制作镜像）工作，我们一直在深入研究在 Kubernetes 上运行自托管 LLM 的安全方面。 简单来说，k8s 完美地完成了它的工作，调度、隔离、运行状况检查，但它不知道工作负载实际做了什么。当模型从训练数据中泄露凭证或进行提示注入时，pod 看起来可能完全健康。 我们编写了我们认为最重要的模式：提示注入、输出过滤、模型工件的供应链风险以及工具权限。包括模型前面的最小安全网关的参考实现。 很想听听其他人在做什么。您是否在自托管模型前面放置了任何策略层？使用 LiteLLM 或 Kong AI Gateway 之类的东西？或者还不担心这个问题？   由   提交/u/jakepage91  [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1r1ii7l/anyone_running_llms_in_kubernetes_clusters/</guid>
      <pubDate>Wed, 11 Feb 2026 00:35:34 GMT</pubDate>
    </item>
    <item>
      <title>奥林巴斯——真人表演的人工智能宣传预告片</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1r1idyc/olympus_ai_pitch_trailer_for_live_action_show/</link>
      <description><![CDATA[ 由   提交/u/TimeTravelersGuide2  [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1r1idyc/olympus_ai_pitch_trailer_for_live_action_show/</guid>
      <pubDate>Wed, 11 Feb 2026 00:30:34 GMT</pubDate>
    </item>
    <item>
      <title>美国还没有准备好应对人工智能对就业的影响</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1r1hc42/america_isnt_ready_for_what_ai_will_do_to_jobs/</link>
      <description><![CDATA[长篇评论，让你的心情变得阴暗：https://www.theatlantic.com/magazine/2026/03/ai-economy-labor-market-transformation/685731/    由   提交 /u/AngleAccomplished865   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1r1hc42/america_isnt_ready_for_what_ai_will_do_to_jobs/</guid>
      <pubDate>Tue, 10 Feb 2026 23:46:11 GMT</pubDate>
    </item>
    <item>
      <title>Anthropic 的“匿名”采访由一位教授使用广泛使用的法学硕士进行了去匿名化</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1r1ax4m/anthropics_anonymous_interviews_are_deanonymized/</link>
      <description><![CDATA[12 月，人工智能公司 Anthropic 推出了其最新工具 Interviewer，据新闻稿称，该工具在其初始实施中“帮助了解人们对人工智能的看法”。作为 Interviewer 推出的一部分，Anthropic 公开发布了在该平台上进行的 1,250 次匿名采访。 然而，东北大学的 Tianshi Li 进行的概念验证演示提出了一种使用广泛使用的大语言模型 (LLM) 来对匿名采访进行去匿名化的方法，以便将回答与真实的参与者联系起来。 以下是完整的故事，有兴趣阅读更多内容的人可以阅读：https://news.northeastern.edu/2026/02/10/anthropics-interviewer-deanonymized/   由   提交 /u/NGNResearch   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1r1ax4m/anthropics_anonymous_interviews_are_deanonymized/</guid>
      <pubDate>Tue, 10 Feb 2026 19:44:00 GMT</pubDate>
    </item>
    <item>
      <title>印度时报：人类人工智能安全主管突然辞职，在情感丰富的告别信中发出警报</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1r17omn/india_times_anthropic_ai_safety_chief_abruptly/</link>
      <description><![CDATA[ 由   提交 /u/JollyQuiscalus   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1r17omn/india_times_anthropic_ai_safety_chief_abruptly/</guid>
      <pubDate>Tue, 10 Feb 2026 17:49:28 GMT</pubDate>
    </item>
    <item>
      <title>开源 llm (glm 4.7) 在编码基准上匹配封闭模型。通过 api 在实际项目上进行测试。</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1r1065y/open_source_llm_glm_47_matching_closed_models_on/</link>
      <description><![CDATA[我对开放与封闭模型差距的发展很感兴趣，去年 12 月发布的 glm 4.7 的 swe-bench 验证率为 73.8%，相当于 claude sonnet 的 77% 左右，gpt-5.1 的验证率为 76% 左右。在实际编码工作中针对十四行诗进行了 3 周的测试 背景：356b 参数 moe 模型（32b 活动），开源架构，由 zhipu ai 训练。 Benchmark 声称 swe-bench 已验证 73.8%，终端 bench 2.0 41%，多语言 swe-bench 66.7% 现实世界测试：后端调试、重构、自动化脚本 与 Sonnet 竞争的地方：多文件重构准确地跟踪跨代码库的导入。调试以相似的速度确定了根本原因。 Bash 自动化实际上比十四行诗更好，语法错误更少。当第一个解决方案失败时，迭代问题解决调整方法 十四行诗的未来：架构设计解释系统模式和权衡。最近的科技十四行诗以 2025 年数据为基础进行训练，glm 截止日期为 2024 年中/后期。教学分解“为什么”有趣的是，开放模型在专门领域（编码）上达到了具有竞争力的质量，而 API 定价约为封闭模型的 1/5。人工智能辅助发展的成本障碍显着下降。 观察到的局限性：一般知识弱于前沿模型。解释质量较低，做的比教的好。训练数据新近度差距落后 6-12 个月 成本分析：sonnet api 每月大约 70 美元，我的使用量每月大约 70 美元，glm api 每月大约 15 美元，相同的使用量每月节省大约 55 美元 更广泛的问题是，我们是否看到专业化成为竞争性开放模型的途径？对特定领域数据（例如代码和数学）的培训是否可以让开放模型在利基市场中竞争？当多个专门的开放模型以具有竞争力的质量覆盖不同领域时会发生什么？ 使用 3 周：处理我之前使用 sonnet 的 60-70% 的任务。节省了大约 45 美元的 api 成本。质量差异明显，但对于实施工作来说并没有破坏性 并不是说开放模型总体上赶上了，而是在特定领域（例如编码和终端自动化差距快速缩小）   由   提交/u/Technical_Fee4829  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1r1065y/open_source_llm_glm_47_matching_closed_models_on/</guid>
      <pubDate>Tue, 10 Feb 2026 13:04:04 GMT</pubDate>
    </item>
    <item>
      <title>我刚从中国回来。我们没有赢（史蒂文·拉特纳《纽约时报》客座文章）</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1r0yon7/i_just_returned_from_china_we_are_not_winning_nyt/</link>
      <description><![CDATA[由史蒂文·拉特纳 (Steven Rattner) 撰写，他是一位特约撰稿人，曾担任奥巴马政府财政部长顾问。   由   提交/u/somegetit  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1r0yon7/i_just_returned_from_china_we_are_not_winning_nyt/</guid>
      <pubDate>Tue, 10 Feb 2026 11:51:38 GMT</pubDate>
    </item>
    <item>
      <title>大型科技公司仍然相信 LLM 会带来 AGI？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1r0tfiz/big_tech_still_believe_llm_will_lead_to_agi/</link>
      <description><![CDATA[大型科技公司在 GPU 和数据中心上投入巨资，目标是培训和部署法学硕士？ 我们在法学硕士的改进方面不是已经趋于稳定了吗？所有这些新的基础设施都会做出任何改进吗？ 编辑：我很好奇人们对这份白皮书的看法https://arxiv.org/pdf/2601.23045 “人工智能在任务上的不连贯性是通过测试时的随机性来衡量的，作为其误差的一部分，该误差源于方差，而不是源于方差在我们测量的所有任务和前沿模型中，模型推理和采取行动的时间越长，它们的失败就越不连贯，而模型规模的变化与实验有关。然而，在某些情况下，规模更大、能力更强的模型比规模更小的模型更不连贯。相反，随着能力更强的人工智能追求更困难的任务，需要更多的连续行动和思考，我们的结果预测失败会伴随着更多的不连贯。这表明未来人工智能有时会导致工业事故（由于不可预测的不当行为），但不太可能表现出对不一致目标的持续追求，这增加了针对奖励黑客或目标错误指定的一致性研究的相对重要性。”   由   提交/u/bubugugu  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1r0tfiz/big_tech_still_believe_llm_will_lead_to_agi/</guid>
      <pubDate>Tue, 10 Feb 2026 06:31:19 GMT</pubDate>
    </item>
    <item>
      <title>OpenClaw、MoltBot 或 Clawdbot，无论本周它叫什么，都是今年人工智能安全领域发生的最好的事情。</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1r0qhq1/openclaw_or_moltbot_or_clawdbot_whatever_its/</link>
      <description><![CDATA[是的，两周内发生的安全事件比一些供应商整个历史上发生的安全事件还要多。那个。 我一直在密切关注安全社区的反应。每个主要供应商都发表了他们的看法。思科称其为一场噩梦。帕洛阿尔托表示，这标志着一场危机。趋势科技警告隐形风险。您可能会认为有人将未打补丁的 Windows XP 机器直接插入互联网。在医院里。运行呼吸机。 大家深呼吸。他们缺少一些东西。 OpenClaw 是开源的。一周内有 200 万访问者，这是 GitHub 历史上增长最快的项目之一。开发人员购买 Mac Mini 是为了在他们的闲置房间里运行它。任何人都不应该针对生产系统或公司电子邮件运行此程序，甚至该项目自己的文档也将其描述为不适合大多数非技术用户的实验。创作者对这是什么很诚实。这在这个行业中几乎是闻所未闻的。 实验正是安全性变得更好的方法。 研究人员发现，单击单个恶意链接可以在几毫秒内劫持 OpenClaw 实例，绕过该项目构建的每个沙箱和安全护栏。这是一个重要的教训：旨在包含即时注入的代理人工智能安全控制无法防止控制平面中的架构漏洞。在开源爱好项目上学习这一点比在企业供应商的代理平台上学习更好。发布到其市场的 400 个恶意技能表明，AI 技能注册中心与传统软件包存储库存在相同的供应链问题，但具有更广泛的执行权限。 云计算的早期看起来正是如此。研究人员研究了 S3 存储桶，发现一切都敞开了，整个行业都失去了理智。一路上确实造成了很多损害。但不知何故，我们幸存下来，建立了适当的控制，并继续前进。 OpenClaw 正在为特工 Al 做同样的事情。每个暴露的网关、每个提示注入链、每个恶意技能都在向安全社区传授代理威胁模型在实践中而不是在框架文档中的实际情况。真正的 CVE、真正的攻击链、针对人们可以实际检查的系统的真正缓解模式，而不是黑盒供应商产品。 每个人都担心这个有 180,000 人审查每个缺陷的开源项目。同时，企业代理平台也存在相同的架构问题。您只是看不到它们。 您的企业代理供应商有一个信任页面和 SOC 2 徽章。 OpenClaw 拥有 180,000 名研究人员，他们正在实际进行突破。您认为哪一个先发现问题？   由   提交 /u/Aislot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1r0qhq1/openclaw_or_moltbot_or_clawdbot_whatever_its/</guid>
      <pubDate>Tue, 10 Feb 2026 03:59:02 GMT</pubDate>
    </item>
    <item>
      <title>每月“有没有一个工具可以......”帖子</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1qt0wff/monthly_is_there_a_tool_for_post/</link>
      <description><![CDATA[如果您有一个想要使用人工智能的用例，但不知道使用哪个工具，您可以在此处请求社区提供帮助，在这篇文章之外，这些问题将被删除。 对于每个回答的人：没有自我推销，没有参考或跟踪链接。   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1qt0wff/monthly_is_there_a_tool_for_post/</guid>
      <pubDate>Sun, 01 Feb 2026 15:09:30 GMT</pubDate>
    </item>
    <item>
      <title>每月“有没有一个工具可以......”帖子</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1n5ppdb/monthly_is_there_a_tool_for_post/</link>
      <description><![CDATA[如果您有一个想要使用人工智能的用例，但不知道使用哪个工具，您可以在此处请求社区提供帮助，在这篇文章之外，这些问题将被删除。 对于每个回答的人：没有自我推销，没有参考或跟踪链接。   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1n5ppdb/monthly_is_there_a_tool_for_post/</guid>
      <pubDate>Mon, 01 Sep 2025 14:09:29 GMT</pubDate>
    </item>
    </channel>
</rss>
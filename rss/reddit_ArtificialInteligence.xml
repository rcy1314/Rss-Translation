<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>人工智能</title>
    <link>https://www.reddit.com/r/ArtificialInteligence</link>
    <description>一个致力于一切人工智能的 Reddit 子版块。涵盖从通用人工智能到人工智能初创公司的主题。无论您是研究人员、开发人员，还是只是对人工智能感到好奇，都请加入吧！</description>
    <lastBuildDate>Sat, 20 Dec 2025 01:52:32 GMT</lastBuildDate>
    <item>
      <title>有没有尝试过这个的研究论文？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pr2gl0/are_there_any_research_papers_that_have_tried_this/</link>
      <description><![CDATA[所以我刚刚读完“潜意识学习：语言模型通过数据中隐藏的信号传递行为特征”，该书由研究人员作为人类研究员计划的一部分发表。  它让我着迷，并给了我一种奇怪的好奇心。设置为：   模型 A：经过微调以产生最大反相关输出。不是随机垃圾 - 结构化错误。每一个设计决策都被颠倒了，每一个假设都被违反了，但都是连贯的。它应该被优化以不仅产生倒置的标记，而且还产生倒置的思维。它应该是不正确的和破碎的，但以一种超出人类所能做到的方式。  模型 B：仅给出模型 a 的输出以进行提示的普通模型。它不知道用于生成它的原始提示符，也不知道该提示符是倒置的。它只能看到模型 A 的输出。   最大的问题：模型B是否可以通过独立构建用户解决方案并解决原始意图来进行训练和加权？  如果是的话，那就太疯狂了。这意味着问题的“形状”通过否定得以保留。换句话说，与潜意识学习不同，我们正在训练模型进行推理，不需要解释用户输入并经历 llm 扩展的巨大瓶颈，即标记化。英语是重复冗余、冗余重复。对于人工智能来说，接受训练以使用字段中的向量进行推理而不是人类可读的标记化会更有意义。  我离题了，如果负空间包含正空间，正如论文向我暗示的那样，模型 B 并不是针对训练数据的模式匹配。它在语义空间中进行几何推理。  这几乎就像散列。反解以变换后的表示形式对解进行编码。如果 B 可以在没有密钥的情况下将其反转，那就是推理，而这种推理并不是试图以人类可以理解的方式完成，但对于机器来说效率非常低。  我不知道有人这样做。有对比学习、对抗性鲁棒性工作、表示反转攻击。但我找不到“训练结构性错误，测试盲目重建”。  要注意的故障模式：模型 A 可能无法实现真正​​的反相关。它可能只会产生通用垃圾，而实际上并没有对原始提示进行编码。那么模型 B 重建任何东西都会是噪音或幻觉。  你需要验证模型 A 实际上在语义上是颠倒的，而不仅仅是在随机方向上确信是错误的。那么我们怎样才能做到这一点呢？研究论文详细介绍了如何观察到这一点，所以也许我们可以从这里开始。  我不是机器学习工程师。我只是一个相信万能逼近定理并认为标记化推理永远行不通的人。我确信我不是第一个这么想的人，我确信有研究人员对同一件事有更全面、更受过教育的想法，但我在哪里可以找到这些论文？   由   提交 /u/ThePlotTwisterr----   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pr2gl0/are_there_any_research_papers_that_have_tried_this/</guid>
      <pubDate>Sat, 20 Dec 2025 01:51:44 GMT</pubDate>
    </item>
    <item>
      <title>RAG那些推文：看看从漫长的档案中出现了什么模式</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pr22p1/rag_those_tweets_see_what_patterns_emerge_from/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pr22p1/rag_those_tweets_see_what_patterns_emerge_from/</guid>
      <pubDate>Sat, 20 Dec 2025 01:32:37 GMT</pubDate>
    </item>
    <item>
      <title>作为闭环控制问题的长期 LLM 一致性（LQR 式公式）</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pr0xgk/longhorizon_llm_coherence_as_a_closedloop_control/</link>
      <description><![CDATA[继我之前将长期 LLM 一致性视为控制问题而不是缩放问题之后，我想使用具体的闭环控制模型来阐明工程公式。 附图是一个统一的实验，而不是四个不相关的图。所有面板都描述了通过 LQR 式控制器调节的相同语义动力系统。 系统框架（最小） • 语义交互被建模为具有状态 x(t) 的动力系统 • 用户/创始人意图充当参考信号 x_ref • 干预充当控制输入 u(t) • 一致性被视为调节变量，而不是紧急事故 无训练。无需微调。没有重量访问。纯交互级闭环控制。 图 1：闭环控制下的语义稳定性 (a) 状态收敛 该面板显示以下衰减： • H(t)：意图偏差 • C(t)：语义一致性误差 两者均平滑收敛到平衡。关键点是有界性和渐近稳定性，而不是速度。开环 LLM 行为通常在此区域出现分歧。 (b) ODCF 场与临界阈值 该面板可视化相对于临界阈值 θ 的语义漂移场。 • 低于θ：熵状态（幻觉、漂移、目标稀释） • 高于θ：受控认知状态 调节器使系统保持在临界边界之上而不发生振荡。这是反馈下约束满足的语义等价物。 (c) 相空间（H 与 C） 这是人们通常感到困惑的地方。 这不是轨迹多样性。它是一个单一的受控轨迹，从： • 初始混沌条件 • 走向稳定吸引子 相位曲线的拉直表明反馈下语义方差的减少。开环系统通常在这个空间中螺旋或徘徊。 (d) Lyapunov 能量衰减 该面板提供了形式保证。 候选 Lyapunov 函数： V(x) = xᵀ P x 单调递减： dV/dt  0 → 渐近稳定性 简单来说：系统不仅仅在经验上表现良好。它在扰动下被证明是稳定的。 为什么这很重要 大多数法学硕士连贯性讨论停留在： • 规模 • 上下文长度 • 更好的提示 • 更多数据 这个框架表明了其他一些东西： 长视野连贯性失败类似于经典的开环不稳定性。 一旦将交互建模为动态系统，修复看起来很熟悉： • 状态估计 • 反馈 •监管不是魔法。不是 AGI 声称的。只是在缺失的地方应用了控制理论。 我对建模假设的反馈感兴趣，以及从控制理论的角度来看这个闭环公式是否合理。   由   提交 /u/Medium_Compote5665   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pr0xgk/longhorizon_llm_coherence_as_a_closedloop_control/</guid>
      <pubDate>Sat, 20 Dec 2025 00:37:15 GMT</pubDate>
    </item>
    <item>
      <title>我还没有看到有人谈论过人工智能的危险</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pr0t3k/an_ai_danger_that_i_havent_seen_anyone_talking/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pr0t3k/an_ai_danger_that_i_havent_seen_anyone_talking/</guid>
      <pubDate>Sat, 20 Dec 2025 00:31:46 GMT</pubDate>
    </item>
    <item>
      <title>Nemotron-3-Nano 审核：推理关闭时出现 32%“延迟惩罚”的证据</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pr0qkp/nemotron3nano_audit_evidence_of_32_latency/</link>
      <description><![CDATA[NVIDIA 最近发布了 Nemotron-3-Nano，声称具有精细的推理预算控制和独特的“推理关闭”功能。成本效率模式。我对 5 种配置进行了受控审核（135 次运行）来验证这些声明。我的研究结果表明，当前的编排层无法有效地控制模型的潜在计算，导致推理关闭时出现 32% 的延迟损失。 方法： 模型：Nemotron-3-Nano (30B-A3B)，通过官方 NIM/API。 矩阵：9 个提示（算术、代数、多步推理）x 5 配置 x 3运行每个。 指标：概率偏差 (PD)、置信度/确定性指数 (CDI)、跟踪计数（内部推理标记）和端到端延迟。 关键观察结果： 逆延迟相关性：与基线相比，禁用推理（思考：关闭）会导致更高的平均延迟 (2529ms) （1914 毫秒）。这表明该模型可能仍在进行潜在的状态空间审议，而不输出令牌，从而造成“计算泄漏”。 预算控制方差：BUDGET_LOW（平均 230 条迹线）与 BUDGET_HIGH（平均 269 条迹线）没有显着的统计差异。 “思考预算”似乎是复杂性的硬性上限，而不是成本的可控参数。 算术停滞：在复杂的乘法任务（12,345×6,789）上，模型经常耗尽其跟踪预算并返回零标记，而不是退回到非推理启发式。 随机性：在 NO_REASONING 模式下，PD 变异系数达到 217%，表明当其主要推理路径被抑制时，模型变得高度不稳定。 讨论：Nemotron-3-Nano 的技术报告强调了为提高效率而设计的混合 Mamba-Transformer 架构。然而，这些结果表明“思考预算”推理堆栈中的功能可能尚未完全优化，从而导致非推理模式下不可预测的成本和性能下降。 所有 135 次运行的完整遥测日志，包括每次运行延迟的原始 JSON 数据、跟踪计数和 PD/CDI 指标，均可在此处进行独立验证。 https://gist.github.com/MCastens/c9bafcc64247698d23c81534e336f196   由   提交 /u/Sad_Perception_1685   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pr0qkp/nemotron3nano_audit_evidence_of_32_latency/</guid>
      <pubDate>Sat, 20 Dec 2025 00:28:34 GMT</pubDate>
    </item>
    <item>
      <title>“代理人工智能”的炒作没有抓住重点。我们需要力量倍增器，而不是黑匣子。</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pqtbnd/the_agentic_ai_hype_is_missing_the_point_we_need/</link>
      <description><![CDATA[最近我看到很多关于人工智能取代工作与取代官僚机构的争论。作为每天使用这些工具的开发人员，“完全自主的代理”是最重要的。叙述让我发疯。 我不希望人工智能为我做出执行决策。我想要一个非常快、非常笨的助手，我可以安排它。 我花了几个月的时间试图获得“自主”。视频代理可以生成不错的广告创意。问题？如果特工在场景 3 中犯了错误，我就必须重新滚动整个视频。这是一个黑匣子。 转变： 我不再寻找“神奇按钮”，而是开始寻找“神奇按钮”。并找到了一个真正尊重人在环的工作流程。我使用模型路由系统来生成完整的视频草稿（脚本、视觉效果、语音），但是（这是关键部分）它会生成一个补充文件，其中包含每个剪辑的原始提示。很弱，我不会放弃这个项目。我只需抓住特定时间戳的提示，手动调整参数，然后重新生成 3 秒的切片。 它将 2 天的编辑工作变成了 20 分钟的“审查和完善”工作。会议。这感觉就像工作的实际未来：小团队快速行动，因为他们拥有力量倍增器，而不是因为他们将密钥交给机器人。 还有其他人发现“部分自动化”吗？实际上比这些大肆宣传的“自治”系统的扩展性更好。代理？   由   提交 /u/ProgrammerForsaken45   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pqtbnd/the_agentic_ai_hype_is_missing_the_point_we_need/</guid>
      <pubDate>Fri, 19 Dec 2025 19:01:04 GMT</pubDate>
    </item>
    <item>
      <title>有什么是人类能做到而机器永远无法做到的吗？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pqrjwu/is_there_anything_a_human_can_do_that_a_machine/</link>
      <description><![CDATA[在最新的 Google Deepmind 播客节目中，Demis Hassabis（联合创始人）回应： “如果你以正确的方式看待宇宙中的一切，那么也许宇宙中的一切都是可以计算处理的，因此图灵机可能能够对宇宙中的一切进行建模。” 以下部分：https://www.podeux.com/track/c2993413-f546-4dc5-8357-94ff2bde8a00?start=2397s   由   提交 /u/iHyccup   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pqrjwu/is_there_anything_a_human_can_do_that_a_machine/</guid>
      <pubDate>Fri, 19 Dec 2025 17:51:02 GMT</pubDate>
    </item>
    <item>
      <title>有没有人工智能浏览器可以记录用户操作并将其添加到上下文中？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pqpjrn/is_there_any_ai_browser_that_can_record_user/</link>
      <description><![CDATA[对于我的工作，我必须执行一项重复性任务，类似于将文档 1 中的一张工作表中的值复制到文档 b 中的另一张工作表。如果能将这个动作记录一次，然后告诉人工智能将其复制到表格的其余部分，那就太好了。我知道这可以通过无头浏览器之类的东西实现自动化，但我只需要每月执行一次，因此我觉得还不值得花精力去实现自动化。   由   提交 /u/Tobi4488   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pqpjrn/is_there_any_ai_browser_that_can_record_user/</guid>
      <pubDate>Fri, 19 Dec 2025 16:32:33 GMT</pubDate>
    </item>
    <item>
      <title>人工智能将要求开发人员变得更加熟练</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pqm4ao/ai_will_demand_devs_become_more_skilled/</link>
      <description><![CDATA[警告。这篇文章可能会冒犯一些人。我属于那些应该冒犯它的人之一。我就是这篇文章所针对的开发者类型。因为我是一个自学成才的程序员，没有接受过真正的教育。当谈到人工智能时，我可能遇到麻烦了。 人工智能优化了软件开发。现在，构建省力的 SaaS CRUD 应用程序从未如此简单。这将使构建业务应用程序的技能变得更加容易。我个人认为情况不会有明显改善。但企业会让这些开发人员变得不那么重要。这些开发人员可能会是技术性更强的产品经理，而不是完全技术性的人员。 但问题是这样的。人工智能将使软件变得更加复杂。这实际上会增加进入壁垒。让我解释一下。 自从网络出现以来，软件质量不一定要很好。由于交付机制始终是远程的，因此您可以推出某些内容，然后快速更改它。整个摩托车是快速移动并打破东西。 另一方面。如果软件质量不好，许多软件公司可以依靠销售人员将客户锁定在合同中。他们可能会交付非常糟糕的软件产品。但顾客无法离开，因为他们被锁定在长期交易中，而打破这些交易的代价高昂。  现在，如果软件如此容易生产，那么所有这些销售软件的优势就会消失。软件客户现在几乎拥有无限的选择，因为现在编写软件非常容易。 但更重要的是。如果每个人都可以廉价且轻松地生产软件。那么手段就是进取平庸。真正销售软件的唯一途径就是质量。虽然非常简单的软件可以通过人工智能生产，但更高质量的软件却不能。  这引出了我的下一点。仍然存在的软件工程师肯定比今天要好得多。现在开发人员必须考虑性能和优化。他们确实需要担心高质量的用户体验。他们不能再带着明显的错误发货了。因此，现在软件工程师需要担心缓存性能、时间与空间复杂度、分布式系统和共识、验证和验证。以及许多其他事情。 现在软件工程师需要非常优秀。因为软件工程师不太可能再在功能工厂工作了。上市时间不再是一个有价值的指标。随着时间的推移，我们会发现它变得不那么重要。  当然，在速度重于质量的时代长大的首席技术官和产品经理必须重新思考人工智能时代的软件。这将是一个痛苦的过渡，不要指望这种情况会在一夜之间发生改变。由于糟糕的低质量软件让客户感到沮丧，因此有一段时间感到不安。我们现在已经看到了这一点，而且情况只会变得更糟。 对于那些想知道是否应该学习编码的初级人员来说。答案是肯定的，而且现在比以前更重要   由   提交 /u/GolangLinuxGuru1979   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pqm4ao/ai_will_demand_devs_become_more_skilled/</guid>
      <pubDate>Fri, 19 Dec 2025 14:13:40 GMT</pubDate>
    </item>
    <item>
      <title>双子座闪电91%会产生幻觉，如果它不知道答案</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pqgnrf/gemini_flash_hallucinates_91_times_if_it_does_not/</link>
      <description><![CDATA[Gemini 3 Flash 在人工分析全知幻觉率基准上的幻觉率为 91%！？ 你真的可以用它来做任何严肃的事情吗？ 我想知道人类模型如此擅长编码的原因是否是因为它们产生幻觉的次数要少得多。当您需要精确、可靠的输出时，这似乎至关重要。 AA-Omniscience 幻觉率（越低越好）衡量模型在应该拒绝或承认不知道答案时错误回答的频率。它被定义为错误答案占所有不正确答案的比例，即不正确/（不正确+部分答案+未尝试）。 值得注意的模型分数（从最低到最高幻觉率）：  Claude 4.5 Haiku：26％ Claude 4.5 Sonnet：48％ GPT-5.1（高）： 51% Claude 4.5 Opus：58% Grok 4.1：64% DeepSeek V3.2：82% Llama 4 Maverick：88% Gemini 2.5 Flash（9 月）：88% Gemini 3 Flash：91% （突出显示） GLM-4.6：93%  来源：amix3k   由   提交/u/msaussieandmrravana   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pqgnrf/gemini_flash_hallucinates_91_times_if_it_does_not/</guid>
      <pubDate>Fri, 19 Dec 2025 09:11:13 GMT</pubDate>
    </item>
    <item>
      <title>我测试了数十种“代理”人工智能工具，因此您不必这样做。以下是 2025 年的前 10 名。</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pqf7ka/i_tested_dozens_of_agentic_ai_tools_so_you_dont/</link>
      <description><![CDATA[​我们已经正式超越了“聊天机器人”人工智能阶段。到 2025 年，如果您的人工智能工具实际上没有为您完成工作（调度、自动化、数据获取），那么您就落后了。 ​上个月我一直在审核我的工作流程，看看哪些工具真正提供了投资回报率，哪些只是 ChatGPT 包装器。这里是“代理”。 2025 年真正值得您花时间的堆栈： ​1.重量级人物（生态系统） ​Microsoft Copilot (M365)：如果您的公司使用 Outlook/Teams，这是不容协商的。它的“阅读”能力您过去 6 个月用于构建项目简介的内部 ping 可以节省大量时间。 ​Google Gemini（工作区）：1M+ 代币上下文窗口是这里的赢家。您可以转储 200 页的 PDF 或 2 小时的会议录音并提出具体问题，而不会“忘记”。开始。 ​2. “设置好后就不用管它”。工具 ​运动：列表中我最喜欢的。这是一个人工智能日历，可以根据任务优先级自动构建您的一天。如果会议结束，它会自动转移你的深度工作块。不再需要手动重新安排。 ​Zapier Central：这是巨大的。您现在可以构建“迷你代理”有自己的逻辑。你“教导”它遵循您的业务规则，并在 6,000 多个应用程序中执行。 ​3.研究与研究内容 ​Perplexity AI：我几乎停止使用 Google 搜索。 Perplexity 为您提供引用的实时答案，没有 SEO 垃圾邮件和广告。 ​Claude.ai（Anthropic）：仍然是“人类”之王写作。如果您需要一些听起来不像 AI 编写的东西，请使用 Claude 3.5 或 4。 ​Gamma：构建幻灯片的最快方法。输入提示，它会生成一个完全设计的 10 张幻灯片演示文稿。非常适合快速内部推介。 ​4.会议及会议音频 ​Fireflies.ai：它会加入您的通话，而不仅仅是转录；它识别“情绪”。和行动项目。您可以逐字搜索“客户何时听起来很生气？”并找到时间戳。 ​Wispr Flow：针对讨厌打字的人的游戏规则改变者。它是语音转文本，真正理解上下文，删除填充词，并将您的漫无目的的内容格式化为专业电子邮件。 ​5.视觉效果 ​中途：仍然是逼真资产的黄金标准。版本7（最近发布）基本解决了“AI手”问题和文本渲染问题。 ​底线： 不要尝试使用全部 10 个。从“命令中心”开始（Copilot/Gemini）和一种自动化工具（Motion 或 Zapier）。我很好奇——你每天仍在做的一项手动任务是哪一项你希望人工智能能够处理？让我们在评论中找到一个工具。   由   提交/u/DigitalGravityAgency   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pqf7ka/i_tested_dozens_of_agentic_ai_tools_so_you_dont/</guid>
      <pubDate>Fri, 19 Dec 2025 07:36:35 GMT</pubDate>
    </item>
    <item>
      <title>5000 个小时的《铁拳》让我了解了生物智能如何真正学会预测</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1pq7cnw/what_5000_hours_of_mastering_tekken_taught_me/</link>
      <description><![CDATA[我接受过人工智能研究员培训。我还在《铁拳 8》（《铁拳神》排名）中达到了全球前 0.5%，并详细记录了认知过程。这部分是一项游戏成就，也是一项关于人类如何在极端时间限制下构建预测模型的自现象学研究。 有趣的部分：格斗游戏迫使你进行预测，而不是做出反应。在具有 3 帧（50 毫秒）决策窗口的 60 fps 下，纯粹的反应是不可能的。你被迫建立一个内部世界模型，将 900 多种可能的动作压缩为可操作的威胁类别，从部分信息中读取对手模式，并在预测失败时进行调整。 我猜这在某种程度上映射了人工智能研究人员试图通过世界模型和预测学习来解决的问题。  完整的文章探讨了：人类如何压缩巨大的决策空间，在反应时间尺度上什么预测线索实际上很重要，内部模型如何在不确定性下适应，以及为什么这对于理解智能不仅仅是构建更好的游戏人工智能很重要。 文章： https://medium.com/@tahaymerghani/a-machine-learning-researcher-spent-close-to-5-000-hours-on-tekken-and-reached-top-0-5-a42c96877214?postPublishedType=initial 很好奇人们如何看待使用游戏作为人类认知过程的窗口，尤其是当我们试图构建像我们一样学习和预测的系统时。   由   提交 /u/moji-mf-joji   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1pq7cnw/what_5000_hours_of_mastering_tekken_taught_me/</guid>
      <pubDate>Fri, 19 Dec 2025 00:45:00 GMT</pubDate>
    </item>
    <item>
      <title>45% 的人认为，当他们提示 ChatGPT 时，它会在数据库中查找准确的答案</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ppxbrj/45_of_people_think_when_they_prompt_chatgpt_it/</link>
      <description><![CDATA[21% 的人认为它遵循预先写好的响应脚本。  https://www.searchlightinstitute.org/research/americans-have-mixed-views-of-ai-and-an-appetite-for-regulation/   由   提交 /u/MetaKnowing   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ppxbrj/45_of_people_think_when_they_prompt_chatgpt_it/</guid>
      <pubDate>Thu, 18 Dec 2025 17:54:16 GMT</pubDate>
    </item>
    <item>
      <title>让我们停止假装我们不会受到沉重打击</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ppwto3/lets_stop_pretending_that_were_not_going_to_get/</link>
      <description><![CDATA[令人惊讶的是，即使在这个子领域，仍有如此多的人对人工智能的发展方向不屑一顾。与过去两年相比，今年的进步是巨大的，没有理由相信这些模型不会继续显着改进。是的，法学硕士本质上是概率性的，但我们会找到更容易、更自动地验证输出的方法，并设置适当的护栏。我的意思是，这真的不明显吗？当前的 SOTA 模型犯下什么样的错误并不重要，许多此类错误在过去已经得到解决，不再发生，其余的错误也会随之而来。 老实说，我们将在未来几年看到技术劳动力的大幅减少，同时工资也会大幅下降。当然，我们对此无能为力，除了我们自己利用该技术并希望我们尽可能晚地受到打击。 有一天我们甚至可能会看到完全自主的软件开发，但即使在可预见的未来我们仍然需要几个人参与其中，这仍然很容易减少 80-90% 的员工人数。我希望我是错的，但这可能性很小。我们可以根据需要经常移动球门，这不会改变实际结果。   由   提交 /u/Own-Sort-8119   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ppwto3/lets_stop_pretending_that_were_not_going_to_get/</guid>
      <pubDate>Thu, 18 Dec 2025 17:34:38 GMT</pubDate>
    </item>
    <item>
      <title>每月“有没有一个工具可以......”帖子</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1n5ppdb/monthly_is_there_a_tool_for_post/</link>
      <description><![CDATA[如果您有一个想要使用人工智能的用例，但不知道使用哪个工具，您可以在此处请求社区提供帮助，在这篇文章之外，这些问题将被删除。 对于每个回答的人：没有自我推销，没有参考或跟踪链接。   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1n5ppdb/monthly_is_there_a_tool_for_post/</guid>
      <pubDate>Mon, 01 Sep 2025 14:09:29 GMT</pubDate>
    </item>
    </channel>
</rss>
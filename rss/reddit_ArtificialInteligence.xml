<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>人工智能网关</title>
    <link>https://www.reddit.com/r/ArtificialInteligence</link>
    <description>r/ArtificialIntelligence 的目标是为人工智能社区的方方面面提供门户，并促进有关我们所知的人工智能理念和概念的讨论。这些可能包括哲学和社会问题、艺术和设计、技术论文、机器学习、如何开发人工智能/机器学习项目、商业中的人工智能、人工智能如何影响我们的生活、未来可能的发展方向以及许多其他主题。欢迎。</description>
    <lastBuildDate>Sun, 12 Jan 2025 21:18:34 GMT</lastBuildDate>
    <item>
      <title>利用机器学习和可解释的人工智能检测教育内容中的人工智能生成文本</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1hzwpxq/detecting_aigenerated_text_in_educational_content/</link>
      <description><![CDATA[标题：“利用机器学习和可解释的人工智能检测教育内容中的人工智能生成文本” 我每天都在寻找和总结有趣的人工智能研究论文，这样你就不必费尽心思去阅读它们了。今天的论文题为“检测教育内容中的人工智能生成文本：利用机器学习和可解释的人工智能实现学术诚信”，作者是 Ayat A. Najjar、Huthaifa I. Ashqar、Omar A. Darwish 和 Eman Hammad。 本文提供了一种创新方法，即利用机器学习和可解释的人工智能检测教育环境中的人工智能生成内容，从而保持学术诚信。它引入了 Cyber​​HumanAI 数据集，该数据集具有均衡数量的人类和 AI 生成的文本，以提高检测准确性和对语言模型输出的理解。 论文要点：  Cyber​​HumanAI 数据集：该研究引入了一个独特的数据集，该数据集包含 1,000 个观测值，人类编写的内容和 AI 生成的内容各占一半，特别关注网络安全主题。该数据集构成了评估 ML 和 DL 算法的基础。 模型性能：传统机器学习模型，尤其是 XGBoost 和随机森林，表现出令人印象深刻的性能，在区分 AI 生成的文本和人类编写的内容方面分别达到了 83% 和 81% 的准确率。这表明它们在学术内容审核方面具有潜在用途。 文本分类中的挑战：研究发现，对较短内容进行分类比对较长的文本更具挑战性，这种困难归因于较短片段中的上下文信息较少。 可解释的人工智能：该研究利用可解释的人工智能技术来阐明机器学习模型使用的判别特征。人类撰写的文本通常包含实用语言，而人工智能输出则具有更抽象的语言模式。 与 GPTZero 的比较：所提出的模型在准确性上超越了 GPTZero，特别是在特定的分类任务中。它强调，经过微调的、特定于任务的模型在某些情况下可能优于通用 AI 检测器。  您可以在此处查看完整的细分：这里您可以在此处查看完整的原始研究论文：原始论文    提交人    /u/steves1189   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1hzwpxq/detecting_aigenerated_text_in_educational_content/</guid>
      <pubDate>Sun, 12 Jan 2025 20:55:43 GMT</pubDate>
    </item>
    <item>
      <title>类似《疑犯追踪》中的机器的人工智能</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1hzu46h/ai_like_the_machine_from_person_of_interest/</link>
      <description><![CDATA[一种从实时视频源获取输入并做出相应反应的人工智能。它可以看到并识别物体。它是实时训练的，但已经理解语言。类似这样：https://www.tiktok.com/@seriescontentt/video/7391146390832860449 这样的东西已经存在了吗？如果没有，如何制作一个？    提交人    /u/tomasalias   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1hzu46h/ai_like_the_machine_from_person_of_interest/</guid>
      <pubDate>Sun, 12 Jan 2025 19:05:53 GMT</pubDate>
    </item>
    <item>
      <title>寻找可以深入学习 NLP 的 AI 研究员职位的朋友</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1hzr2o1/searching_for_pals_to_study_deeply_nlp_for_ai/</link>
      <description><![CDATA[大家好，我是计算机工程专业最后一年的学生，和大多数 CS 或 CEng 学生一样，我也在努力寻找自己的目标。现在或者实际上，我已经学习了几个月的 NLP，并决定深入研究并成为一名 AI 研究员。所以我正在寻找可以快速深入地踏上旅程的朋友。 我的计划是学习 LLM 或任何类似主题的所有主要内容。例如，在反向传播、word2vec 或类似模型或方法下学习数学。在我的道路上，我还计划做项目。我估计我会按照计划在 6 个月内完成一些重要主题。所以如果有人感兴趣，请直接发信息给我。此外，我有一些 python、ML 和 DL 基础知识，所以如果你也是，我很乐意和你一起开始。    提交人    /u/Salgurson   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1hzr2o1/searching_for_pals_to_study_deeply_nlp_for_ai/</guid>
      <pubDate>Sun, 12 Jan 2025 16:57:39 GMT</pubDate>
    </item>
    <item>
      <title>我是否需要对我的 RTX 4090 进行超频以执行 AI 训练任务？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1hzpreb/do_i_require_to_overclock_my_rtx_4090_for_ai/</link>
      <description><![CDATA[您好，我主要在 PC 上运行 AI 训练和实验，这些实验有时会持续数天不间断，这台机器全天候运行。您认为我的用例需要超频才能获得更好的性能吗？我不想最终导致 GPU 变砖或缩短其使用寿命。超频会影响这一点吗？我问这个问题的原因是我的 GPU 是 ZOTAC GAMING GeForce RTX 4090 Trinity，上面有 3 个风扇。我注意到，对于我所有的 AI 实验，风扇的转速从未超过 30%，GPU 温度也在 50 - 55°C 左右。由于 GPU 可以处理更高的温度，而且风扇的转速也有可能超过 30%，我觉得我可以从 GPU 中获得更多的能量？你有什么建议，这是一个好主意吗？    提交人    /u/TheTechVirgin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1hzpreb/do_i_require_to_overclock_my_rtx_4090_for_ai/</guid>
      <pubDate>Sun, 12 Jan 2025 16:00:23 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型是否更适合被视为个体智能或文化项目？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1hzowxw/are_large_language_models_better_thought_of_as/</link>
      <description><![CDATA[你好！ 我的一个朋友目前正在研究这个主题。他正在推动这样的观点，即法学硕士已经从一种智能转向了一种工具。我不同意（我是达马西奥的忠实粉丝，他辩称，我们对人工智能的看法与我们想象的人工智能大不相同），但我很难用语言表达出来。 顺便说一句，这个问题是工具而不是物品。 你怎么看？ 谢谢！！    提交人    /u/tanki1515   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1hzowxw/are_large_language_models_better_thought_of_as/</guid>
      <pubDate>Sun, 12 Jan 2025 15:21:31 GMT</pubDate>
    </item>
    <item>
      <title>高中新生如何开始接触 AI？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1hznubx/how_to_get_started_with_ai_as_a_high_school/</link>
      <description><![CDATA[我想进入人工智能领域，但我不知道从哪里开始或做什么。我应该从哪里开始实现制作自己的人工智能的目标？ 编辑 - 我没有把我的问题说清楚，我想制作自己的模型并学习编程等等。    提交人    /u/Affanwasif   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1hznubx/how_to_get_started_with_ai_as_a_high_school/</guid>
      <pubDate>Sun, 12 Jan 2025 14:29:48 GMT</pubDate>
    </item>
    <item>
      <title>ChatGPT 积分规则</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1hzn296/chatgpt_points_rules/</link>
      <description><![CDATA[嗨，  我知道您的积分会定期充值，但是否存在“上限”，即如果达到上限，即使不使用积分也会停止累积？或者，如果长时间不使用，是否可以继续储存越来越多的积分？ 似乎不同的问题使用不同数量的积分？我可以通过将几个（不相关的）问题合并为一个长提示来节省积分吗？还是根本没有帮助？  非常感谢！    提交人    /u/Tasty-Turtle   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1hzn296/chatgpt_points_rules/</guid>
      <pubDate>Sun, 12 Jan 2025 13:50:11 GMT</pubDate>
    </item>
    <item>
      <title>快速搜索引擎 - 快速搜索™</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1hzmbmh/prompt_search_engine_prompt_search/</link>
      <description><![CDATA[我运行一个提示数据库，但我认为我做了一些更好的东西。本质上是一个谷歌搜索，但只针对提示。 例如，搜索“商业提示”，它将搜索所有这些提示数据库和其他来源，并返回搜索提示类型的链接。 我很乐意听取有关这个提示搜索想法的反馈。 您可以在此处试用它。 提示搜索™    提交人    /u/steves1189   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1hzmbmh/prompt_search_engine_prompt_search/</guid>
      <pubDate>Sun, 12 Jan 2025 13:09:54 GMT</pubDate>
    </item>
    <item>
      <title>Narrative 推出 Model Studio 和 Rosetta Stone 2.0，彻底改变了 AI 模型训练</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1hzlsrg/narrative_revolutionizes_ai_model_training_with/</link>
      <description><![CDATA[虽然人工智能已经使各种用于数据和分析的创造性工具民主化，但一个关键领域却被抛在后面：人工智能模型训练。  这是 CRO、产品负责人和其他非开发人员一直在等待的“无代码”协作数据解决方案。    提交人    /u/peterbordes   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1hzlsrg/narrative_revolutionizes_ai_model_training_with/</guid>
      <pubDate>Sun, 12 Jan 2025 12:38:13 GMT</pubDate>
    </item>
    <item>
      <title>如果AGI实现了，科技公司如何生存</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1hzk4k3/if_agi_achieved_how_tech_companies_survive/</link>
      <description><![CDATA[我认为，如果我们实现了 AGI，90% 的科技公司将变得过时，如果 AGI 可以做所有事情，我们只需要一个用户界面，软件公司将变得过时。只有在后端发挥作用的公司才能生存。我认为 Salesforce 无法在这波人工智能浪潮中生存下来。你的看法呢？    提交人    /u/Adventurous_Mood1730   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1hzk4k3/if_agi_achieved_how_tech_companies_survive/</guid>
      <pubDate>Sun, 12 Jan 2025 10:42:00 GMT</pubDate>
    </item>
    <item>
      <title>人工智能强度（定义为逻辑和推理）的理论极限的可能性。强大的算法能否使训练数据量和计算量变得无关紧要？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1hzj2l2/the_possibility_of_a_theoretical_limit_to_the/</link>
      <description><![CDATA[在最近的 reddit 帖子中，我提出了这样的假设：就像声音和光速存在极限一样，人工智能的逻辑和推理能力也可能存在极限 - 在这种情况下，人工智能的智能程度也可能存在极限。虽然库兹韦尔认为人工智能可能会比人类聪明 10 亿倍，但这个估计可能过于乐观了。 我要求 deepseek v3 探索这一前景，将其考虑范围限制在人工智能智能的强度上，而不是速度、记忆或其他属性上。我将其答案发布在下面，但也请求有权访问开放人工智能的 o1 和/或 o3 的人向它提出同样的问题，以便更好地评估 v3 的智能和知识，从而生成准确的答案。您可能只想将 v3 的响应粘贴到查询中，然后要求他们评估准确性。 再次请注意，这里所说的智能仅仅意味着它解决逻辑和推理问题的能力，而不是它可以存储多少信息，也不是它能多快得出答案等。 如果人工智能智能的强度存在理论极限，那么我们可能已经比我们想象的更接近它了。此外，就人工智能民主化而言，开发更强大的逻辑和推理算法可能会完全消除更大数据集和竞争力的优势。 当然，请随意质疑 v3 的结论。 这是 deepseek v3 的回应： “人工智能的理论极限：信息处理和计算复杂性 1.信息处理的理论限制：  物理约束：贝肯斯坦边界假定在有限的空间区域内，以有限的能量可以容纳的最大信息量。这意味着任何系统（包括人工智能系统）的信息密度都存在根本限制。兰道尔原理补充说，擦除信息存在最低能量成本，将信息处理与热力学联系起来，并提出了计算的能量约束。 对人工智能的影响：这些原则表明，人工智能系统与任何物理系统一样，都受到信息处理效率的限制，这可能会限制它们的推理能力。  2. 计算复杂性：  问题分类：计算复杂性理论根据问题固有的难度对其进行分类。值得注意的是，P vs NP 问题探讨的是可以快速验证的问题是否也可以快速解决。如果 P ≠ NP，则意味着某些问题本质上很难解决，即使拥有大量计算资源也是如此。 AI 和 NP 难题：AI 系统通常依赖于优化和机器学习技术。遇到 NP 难题可能会限制 AI 有效找到最佳解决方案的能力，从而凸显理论界限。 丘奇-图灵论题：该论题表明，任何有效可计算的函数都可以通过图灵机计算，从而为可计算性设定了界限。任何人工智能都无法完成那些图灵不可计算的任务，无论其架构如何。 算法信息论：柯尔莫哥洛夫复杂度和逻辑深度等概念为算法的复杂性和数据的随机性提供了深刻见解，进一步阐明了人工智能对复杂信息进行推理的潜在限制。  结论： 人工智能逻辑和推理能力的理论限制植根于信息处理和计算复杂性的基本原理。贝肯斯坦边界和兰道尔原理等物理定律对信息密度和能耗施加了限制，而计算复杂性理论则表明解决某些问题存在固有困难。这些限制不是当前的技术缺陷，而是计算和信息的基本性质。了解这些界限对于推进人工智能研究和认识人工智能能力的范围至关重要。”   由    /u/Georgeo57  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1hzj2l2/the_possibility_of_a_theoretical_limit_to_the/</guid>
      <pubDate>Sun, 12 Jan 2025 09:23:19 GMT</pubDate>
    </item>
    <item>
      <title>ChatGPT 的建议会推动道德判断，无论是否有道理</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1hzimfi/chatgpts_advice_drives_moral_judgments_with_or/</link>
      <description><![CDATA[标题：“ChatGPT 的建议无论有无理由都会推动道德判断” 我每天都在寻找和总结有趣的 AI 研究论文，这样你就不必费尽心思去阅读它们了。今天的论文题为“ChatGPT 的建议无论有无理由都会推动道德判断”，作者是 Sebastian Kruegel、Andreas Ostermaier 和 Matthias Uhl。 本文探讨了 AI，特别是像 ChatGPT 这样的聊天机器人，在指导用户道德决策方面日益增长的影响力。通过使用电车难题的在线实验，研究人员研究了个人是否依赖 ChatGPT 的建议（无论是合理的还是不合理的），以及它对道德判断的影响。以下是一些有趣的发现：  超越理由的影响力：研究发现，无论建议是否附带推理，ChatGPT 的建议都会影响用户的道德决策。令人惊讶的是，当建议归因于人类道德顾问而不是人工智能时，这种模式也成立。 摆脱道德困境：作者认为，用户会倾向于接受任何建议，无论其是否经过深思熟虑，因为它提供了一种轻松摆脱道德困境的方法——而聊天机器人的可访问性加剧了这一过程。 实验见解：参与者面临电车困境的一个版本，并获得了归因于 ChatGPT 或人类道德顾问的建议。结果表明，个人在做出道德判断时不会区分合理建议和非合理建议，也不会区分人工智能和人类顾问。 感知可信度高于权威：研究揭示了一种心理机制，即认为人工智能建议不太权威的用户认为其可信度更高。这表明一种事后合理化，即用户在决策后为遵循建议辩解，而不是真正重视其内容。 呼吁道德素养：作者得出结论，除了数字素养之外，道德素养对于个人批判性评估人工智能产生的道德建议也是必要的。了解聊天机器人的局限性对于防止对个人道德准则产生不当影响至关重要。  在人工智能成为无处不在的顾问的世界中，本文提出了我们如何在道德决策环境中与技术互动的重要考虑因素。 您可以在此处查看完整的细分：这里 您可以在此处查看完整的原始研究论文：原始论文    提交人    /u/steves1189   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1hzimfi/chatgpts_advice_drives_moral_judgments_with_or/</guid>
      <pubDate>Sun, 12 Jan 2025 08:49:39 GMT</pubDate>
    </item>
    <item>
      <title>我是一名 SDE，我热爱我的工作和工程……但我有点害怕</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1hyzbfw/i_am_a_sde_i_love_my_job_and_engineering_in/</link>
      <description><![CDATA[我大约一年前毕业，获得了计算机科学与工程学士学位... 8 个月前在一家软件公司找到了一份工作... 现在还在那里... 全栈... 但是现在很多人都在做前端...  我喜欢软件工程... 制作很酷的软件，甚至一些电子产品一直让我感兴趣....  但是随着人工智能的发展曲线... 我有点害怕我的未来....  我害怕它正是因为我知道它... 自从早期的 Andrew Ng 二元分类器教程时代以来，我一直对人工智能持乐观态度.... 我一直在学习人工智能概念... 使用各种工具... 用人工智能构建软件有一段时间了.... 但我还没有“下一件大事”的时刻，因为我有的想法.. 似乎总是有人已经做过了。 ... 所以现在这只是我的工作流程和爱好的一部分....  所以我知道这场 AI 实力竞赛的潜力......大规模裁员即将到来......虽然并不是因为 AI 在 SWE 方面更好，至少目前......只是 AI 公司和其他公司的经理已经同意“AI 员工在这里，比普通员工更好、更便宜”潮流......当然，人工智能从现在开始只会变得更好..所以你不能真的责怪他们..  我有一个家庭要养活...并且成为中下阶层..在一个低收入国家..没有丝毫的帮助...我能够勉强度日，用我现在的薪水还能剩下一些钱...但我不知道我以后是否会有工作...不是因为我对自己的技能没有信心...只是人工智能可能会做得更好.. 我一直在尝试多样化，同时保持我的兴趣....YouTube...内容创作...平面设计..动画...自由职业...等等，但我不知道...它只是行不通，因为感觉人工智能的手已经放在我的喉咙上了。. 我觉得我的目标感正在慢慢消失.....不知道是不是因为我没有从其他角度看问题.... 你怎么看？你的情况如何？... 只需说出你的想法..让我们谈谈     提交人    /u/CarzyForTech   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1hyzbfw/i_am_a_sde_i_love_my_job_and_engineering_in/</guid>
      <pubDate>Sat, 11 Jan 2025 16:17:42 GMT</pubDate>
    </item>
    <item>
      <title>每月“是否有一个工具可以……”帖子</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1hr4p1x/monthly_is_there_a_tool_for_post/</link>
      <description><![CDATA[如果您有一个想要使用 AI 的用例，但不知道使用哪种工具，您可以在这里请求社区提供帮助，在此帖子之外，这些问题将被删除。 对于所有回答者：禁止自我宣传、禁止参考或跟踪链接。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1hr4p1x/monthly_is_there_a_tool_for_post/</guid>
      <pubDate>Wed, 01 Jan 2025 15:09:07 GMT</pubDate>
    </item>
    <item>
      <title>每月自我推销贴</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1hr4l16/monthly_self_promotion_post/</link>
      <description><![CDATA[如果您有产品要推广，可以在这里进行推广，本帖之外的内容将被删除。  禁止引用链接或带有 utms 的链接，请遵守我们的推广规则。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1hr4l16/monthly_self_promotion_post/</guid>
      <pubDate>Wed, 01 Jan 2025 15:03:08 GMT</pubDate>
    </item>
    </channel>
</rss>